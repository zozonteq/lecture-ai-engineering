import os
import torch
from transformers import pipeline
import time
import traceback
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import uvicorn
import nest_asyncio
from pyngrok import ngrok

# --- è¨­å®š ---
# ãƒ¢ãƒ‡ãƒ«åã‚’è¨­å®š
MODEL_NAME = "google/gemma-2-2b-jpn-it"  # ãŠå¥½ã¿ã®ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›´å¯èƒ½ã§ã™
print(f"ãƒ¢ãƒ‡ãƒ«åã‚’è¨­å®š: {MODEL_NAME}")

# --- ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚¯ãƒ©ã‚¹ ---
class Config:
    def __init__(self, model_name=MODEL_NAME):
        self.MODEL_NAME = model_name

config = Config(MODEL_NAME)

# --- FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®šç¾© ---
app = FastAPI(
    title="ãƒ­ãƒ¼ã‚«ãƒ«LLM APIã‚µãƒ¼ãƒ“ã‚¹",
    description="transformersãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ãŸã‚ã®API",
    version="1.0.0"
)

# CORSãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã‚’è¿½åŠ 
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®šç¾© ---
class Message(BaseModel):
    role: str
    content: str

# ç›´æ¥ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆ
class SimpleGenerationRequest(BaseModel):
    prompt: str
    max_new_tokens: Optional[int] = 512
    do_sample: Optional[bool] = True
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9

class GenerationResponse(BaseModel):
    generated_text: str
    response_time: float

# --- ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®é–¢æ•° ---
# ãƒ¢ãƒ‡ãƒ«ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
model = None

def load_model():
    """æ¨è«–ç”¨ã®LLMãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    global model  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã«å¿…è¦
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
        pipe = pipeline(
            "text-generation",
            model=config.MODEL_NAME,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device=device
        )
        print(f"ãƒ¢ãƒ‡ãƒ« '{config.MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸ")
        model = pipe  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°
        return pipe
    except Exception as e:
        error_msg = f"ãƒ¢ãƒ‡ãƒ« '{config.MODEL_NAME}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}"
        print(error_msg)
        traceback.print_exc()  # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›
        return None

def extract_assistant_response(outputs, user_prompt):
    """ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‹ã‚‰ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’æŠ½å‡ºã™ã‚‹"""
    assistant_response = ""
    try:
        if outputs and isinstance(outputs, list) and len(outputs) > 0 and outputs[0].get("generated_text"):
            generated_output = outputs[0]["generated_text"]
            
            if isinstance(generated_output, list):
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å ´åˆ
                if len(generated_output) > 0:
                    last_message = generated_output[-1]
                    if isinstance(last_message, dict) and last_message.get("role") == "assistant":
                        assistant_response = last_message.get("content", "").strip()
                    else:
                        # äºˆæœŸã—ãªã„ãƒªã‚¹ãƒˆå½¢å¼ã®å ´åˆã¯æœ€å¾Œã®è¦ç´ ã‚’æ–‡å­—åˆ—ã¨ã—ã¦è©¦è¡Œ
                        print(f"è­¦å‘Š: æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å½¢å¼ãŒäºˆæœŸã—ãªã„ãƒªã‚¹ãƒˆå½¢å¼ã§ã™: {last_message}")
                        assistant_response = str(last_message).strip()

            elif isinstance(generated_output, str):
                # æ–‡å­—åˆ—å½¢å¼ã®å ´åˆ
                full_text = generated_output
                
                # å˜ç´”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã®å ´åˆã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¾Œã®å…¨ã¦ã‚’æŠ½å‡º
                if user_prompt:
                    prompt_end_index = full_text.find(user_prompt)
                    if prompt_end_index != -1:
                        prompt_end_pos = prompt_end_index + len(user_prompt)
                        assistant_response = full_text[prompt_end_pos:].strip()
                    else:
                        # å…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾è¿”ã™
                        assistant_response = full_text
                else:
                    assistant_response = full_text
            else:
                print(f"è­¦å‘Š: äºˆæœŸã—ãªã„å‡ºåŠ›ã‚¿ã‚¤ãƒ—: {type(generated_output)}")
                assistant_response = str(generated_output).strip()  # æ–‡å­—åˆ—ã«å¤‰æ›

    except Exception as e:
        print(f"å¿œç­”ã®æŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        assistant_response = "å¿œç­”ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚"  # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š

    if not assistant_response:
        print("è­¦å‘Š: ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å®Œå…¨ãªå‡ºåŠ›:", outputs)
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¾ãŸã¯ã‚¨ãƒ©ãƒ¼å¿œç­”ã‚’è¿”ã™
        assistant_response = "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    return assistant_response

# --- FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾© ---
@app.on_event("startup")
async def startup_event():
    """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–"""
    load_model_task()  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã¯ãªãåŒæœŸçš„ã«èª­ã¿è¾¼ã‚€
    if model is None:
        print("è­¦å‘Š: èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
    else:
        print("èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

@app.get("/")
async def root():
    """åŸºæœ¬çš„ãªAPIãƒã‚§ãƒƒã‚¯ç”¨ã®ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {"status": "ok", "message": "Local LLM API is runnning"}

@app.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    global model
    if model is None:
        return {"status": "error", "message": "No model loaded"}

    return {"status": "ok", "model": config.MODEL_NAME}

# ç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.post("/generate", response_model=GenerationResponse)
async def generate_simple(request: SimpleGenerationRequest):
    """å˜ç´”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¥åŠ›ã«åŸºã¥ã„ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    global model

    if model is None:
        print("generateã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã¾ã™...")
        load_model_task()  # å†åº¦èª­ã¿è¾¼ã¿ã‚’è©¦ã¿ã‚‹
        if model is None:
            print("generateã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            raise HTTPException(status_code=503, detail="ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚å¾Œã§ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")

    try:
        start_time = time.time()
        print(f"ã‚·ãƒ³ãƒ—ãƒ«ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ä¿¡: prompt={request.prompt[:100]}..., max_new_tokens={request.max_new_tokens}")  # é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯åˆ‡ã‚Šæ¨ã¦

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ã‚­ã‚¹ãƒˆã§ç›´æ¥å¿œç­”ã‚’ç”Ÿæˆ
        print("ãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚’é–‹å§‹...")
        outputs = model(
            request.prompt,
            max_new_tokens=request.max_new_tokens,
            do_sample=request.do_sample,
            temperature=request.temperature,
            top_p=request.top_p,
        )
        print("ãƒ¢ãƒ‡ãƒ«æ¨è«–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”ã‚’æŠ½å‡º
        assistant_response = extract_assistant_response(outputs, request.prompt)
        print(f"æŠ½å‡ºã•ã‚ŒãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”: {assistant_response[:100]}...")  # é•·ã„å ´åˆã¯åˆ‡ã‚Šæ¨ã¦

        end_time = time.time()
        response_time = end_time - start_time
        print(f"å¿œç­”ç”Ÿæˆæ™‚é–“: {response_time:.2f}ç§’")

        return GenerationResponse(
            generated_text=assistant_response,
            response_time=response_time
        )

    except Exception as e:
        print(f"ã‚·ãƒ³ãƒ—ãƒ«å¿œç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

def load_model_task():
    """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯"""
    global model
    print("load_model_task: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã‚’é–‹å§‹...")
    # load_modelé–¢æ•°ã‚’å‘¼ã³å‡ºã—ã€çµæœã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«è¨­å®š
    loaded_pipe = load_model()
    if loaded_pipe:
        model = loaded_pipe  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°
        print("load_model_task: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    else:
        print("load_model_task: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

print("FastAPIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å®šç¾©ã—ã¾ã—ãŸã€‚")

# --- ngrokã§APIã‚µãƒ¼ãƒãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•° ---
def run_with_ngrok(port=8501):
    """ngrokã§FastAPIã‚¢ãƒ—ãƒªã‚’å®Ÿè¡Œ"""
    nest_asyncio.apply()

    ngrok_token = os.environ.get("NGROK_TOKEN")
    if not ngrok_token:
        print("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ãŒ'NGROK_TOKEN'ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        try:
            print("Colab Secrets(å·¦å´ã®éµã‚¢ã‚¤ã‚³ãƒ³)ã§'NGROK_TOKEN'ã‚’è¨­å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
            ngrok_token = input("Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (https://dashboard.ngrok.com/get-started/your-authtoken): ")
        except EOFError:
            print("\nã‚¨ãƒ©ãƒ¼: å¯¾è©±å‹å…¥åŠ›ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
            print("Colab Secretsã‚’ä½¿ç”¨ã™ã‚‹ã‹ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚»ãƒ«ã§`os.environ['NGROK_TOKEN'] = 'ã‚ãªãŸã®ãƒˆãƒ¼ã‚¯ãƒ³'`ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            return

    if not ngrok_token:
        print("ã‚¨ãƒ©ãƒ¼: Ngrokèªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä¸­æ­¢ã—ã¾ã™ã€‚")
        return

    try:
        ngrok.set_auth_token(ngrok_token)

        # æ—¢å­˜ã®ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã‚‹
        try:
            tunnels = ngrok.get_tunnels()
            if tunnels:
                print(f"{len(tunnels)}å€‹ã®æ—¢å­˜ãƒˆãƒ³ãƒãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚é–‰ã˜ã¦ã„ã¾ã™...")
                for tunnel in tunnels:
                    print(f"  - åˆ‡æ–­ä¸­: {tunnel.public_url}")
                    ngrok.disconnect(tunnel.public_url)
                print("ã™ã¹ã¦ã®æ—¢å­˜ngrokãƒˆãƒ³ãƒãƒ«ã‚’åˆ‡æ–­ã—ã¾ã—ãŸã€‚")
            else:
                print("ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªngrokãƒˆãƒ³ãƒãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        except Exception as e:
            print(f"ãƒˆãƒ³ãƒãƒ«åˆ‡æ–­ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            # ã‚¨ãƒ©ãƒ¼ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšç¶šè¡Œã‚’è©¦ã¿ã‚‹

        # æ–°ã—ã„ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‹ã
        print(f"ãƒãƒ¼ãƒˆ{port}ã«æ–°ã—ã„ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‹ã„ã¦ã„ã¾ã™...")
        ngrok_tunnel = ngrok.connect(port)
        public_url = ngrok_tunnel.public_url
        print("---------------------------------------------------------------------")
        print(f"âœ… å…¬é–‹URL:   {public_url}")
        print(f"ğŸ“– APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ (Swagger UI): {public_url}/docs")
        print("---------------------------------------------------------------------")
        print("(APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚„ãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«ã“ã®URLã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„)")
        uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’infoã«è¨­å®š

    except Exception as e:
        print(f"\n ngrokã¾ãŸã¯Uvicornã®èµ·å‹•ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        # ã‚¨ãƒ©ãƒ¼å¾Œã«æ®‹ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã‚ˆã†ã¨ã™ã‚‹
        try:
            print("ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šæ®‹ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã¦ã„ã¾ã™...")
            tunnels = ngrok.get_tunnels()
            for tunnel in tunnels:
                ngrok.disconnect(tunnel.public_url)
            print("ngrokãƒˆãƒ³ãƒãƒ«ã‚’é–‰ã˜ã¾ã—ãŸã€‚")
        except Exception as ne:
            print(f"ngrokãƒˆãƒ³ãƒãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­ã«åˆ¥ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {ne}")

# --- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ ---
if __name__ == "__main__":
    # æŒ‡å®šã•ã‚ŒãŸãƒãƒ¼ãƒˆã§ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
    run_with_ngrok(port=8501)  # ã“ã®ãƒãƒ¼ãƒˆç•ªå·ã‚’ç¢ºèª
    # run_with_ngrokãŒçµ‚äº†ã—ãŸã¨ãã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    print("\nã‚µãƒ¼ãƒãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚")