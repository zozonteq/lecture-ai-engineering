{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_6-jdBOXowG"
      },
      "source": [
        "**注意事項**\n",
        "\n",
        "このノートブックは、GPU:「T4」に対応させたものです。\n",
        "「L4」版のノートブックとはモデル等が異なるため、生成される内容が異なることが考えられます。\n",
        "\n",
        "生成される内容と、ノートブックに記載されている説明が一致しない場合があることをご了承ください。\n",
        "\n",
        "生成内容とノートブックの説明をよく見比べ、適宜読み替えながら演習を進めてみてください。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 演習の方針\n",
        "\n",
        "1. **ベースラインモデル評価**  \n",
        "   素のモデルで回答を生成し、講義内容との整合性の低さを観察します。これにより、特別な学習なしでのモデルの限界を確認します。\n",
        "\n",
        "2. **文字起こしデータの活用**  \n",
        "   講義の文字起こしデータを導入し、モデルが講義内容を参照した回答を生成する傾向を観察します。ただし、Retrieval（情報検索）精度の限界から結果は不安定になる可能性があります。\n",
        "\n",
        "3. **チャンク化の導入**  \n",
        "   文字起こしデータをチャンク（小単位）に分割し、より安定して関連コンテンツを取得できるようにします。この段階では文脈理解にまだ課題があることを確認します。\n",
        "\n",
        "4. **Rerankの適用**  \n",
        "   検索結果のランク付けを導入し、より的確で安定した回答を目指します。\n",
        "\n",
        "5. **応用改善手法**  \n",
        "   文字起こしの品質向上のための編集技術や、メタデータの活用による性能向上手法を探ります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPI1pj4mFavt"
      },
      "source": [
        "## 扱う質問\n",
        "\n",
        "「Inference Time Scaling（推論時スケーリング）」に関する質問を取り扱います。これは以下の背景を持つトピックです。\n",
        "\n",
        "- 2024年8月発表の論文「Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters」で提唱された概念\n",
        "- OpenAIのGPT-o1（2024年9月リリース）で実用化され、注目を集めた比較的新しいアプローチ\n",
        "- 2024年度LLM講座の第4回講義でも取り上げられた重要テーマ\n",
        "\n",
        "## 扱うモデル\n",
        "\n",
        "「google/gemma-2-2b-jpn-it」を使用します。このモデルは、リリース時期の関係上、以下の特徴を持ちます。\n",
        "\n",
        "- 「Inference Time Scaling」の概念が広まる前に訓練されており、このトピックに関する知識を持たないと想定される\n",
        "- この特性を活かし、純粋なベースライン評価から各手法の効果を観察する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 演習環境の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM50WAI7GXwC"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install google-colab-selenium\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2PStE0uqM03"
      },
      "outputs": [],
      "source": [
        "# 演習用のコンテンツを取得\n",
        "!git clone https://github.com/matsuolab/lecture-ai-engineering.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXo_kFASXlvp"
      },
      "outputs": [],
      "source": [
        "# HuggingFace Login\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ_NUIftXwLc"
      },
      "outputs": [],
      "source": [
        "# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eTgV8XBPA90"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tV9mO8oXoaM"
      },
      "outputs": [],
      "source": [
        "# モデル(Gemma2)の読み込み\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"google/gemma-2-2b-jpn-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piTdVxTfGcc_"
      },
      "source": [
        "# 1. ベースラインモデル評価\n",
        "**まずはベースモデルがどの程度知識を持っているか確かめる**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBUZ3o6dhMlC"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZXyEnZ3lrBd"
      },
      "outputs": [],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSCNnRf9pJif"
      },
      "source": [
        "## 結果 (ベースモデル)\n",
        "\n",
        "「google/gemma-2-2b-jpn-it」は「Inference Time Scaling」について誤った知識を提示しました：\n",
        "* モデルは従来の「推論時間の短縮」という文脈でInference Time Scalingを解釈しており、これはLLM分野における最新の「Inference Time Scaling」概念（推論時計算資源の最適配分）とは異なる説明になります。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R-hiKNGyJd"
      },
      "source": [
        "# 2. 文字起こしデータの活用\n",
        "## 講義内容をソースとして活用 (RAG導入)\n",
        "\n",
        "モデルの回答の事実性を向上させるためにRetrieval Augmented Generation (RAG)技術を導入します：\n",
        "\n",
        "* **知識ソース**: LLM講座第4講における講師の発言内容\n",
        "* **目的**: モデルに「Inference Time Scaling」に関する正確な知識と文脈を提供し、事実に基づいた回答を促す\n",
        "\n",
        "**初期RAG実装（ベーシックアプローチ）**:\n",
        "* **ドキュメント処理**: 音声認識モデル(speech2text)で書き起こした生テキストをそのまま使用\n",
        "* **分割方法**: 「。」（句点）で区切られた文単位でテキストを分割\n",
        "* **検索手法**: シンプルな類似度ベースの検索でクエリに関連する文を抽出\n",
        "* **制約条件**: モデルの入力トークン制限に収まるよう関連文のみを選択"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47GvcceyObAl"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "emb_model = SentenceTransformer(\"infly/inf-retriever-v1-1.5b\", trust_remote_code=True)\n",
        "# In case you want to reduce the maximum length:\n",
        "emb_model.max_seq_length = 4096"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPwggQfUS5yl"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/lecture-ai-engineering/day3/data/LLM2024_day4_raw.txt\", \"r\") as f:\n",
        "  raw_writedown = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxzKF6L2THIw"
      },
      "outputs": [],
      "source": [
        "# ドキュメントを用意する。\n",
        "documents = [text.strip() for text in raw_writedown.split(\"。\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK4cYURzTHIx"
      },
      "outputs": [],
      "source": [
        "# Retrievalの実行\n",
        "question = \"LLMにおけるInference Time Scalingとは？\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "# 各ドキュメントの類似度スコア\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_v8gx_tTHIx"
      },
      "outputs": [],
      "source": [
        "topk = 5\n",
        "for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "  print(documents[index], \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow0wZy6ETHIx"
      },
      "outputs": [],
      "source": [
        " #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n",
        "references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_4dkHGKTPr-"
      },
      "outputs": [],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn7tih0RTTzr"
      },
      "source": [
        "## 結果 (初期RAG実装)\n",
        "\n",
        "講義内容のドキュメントを追加したにもかかわらず、モデルの回答には依然として以下の問題が見られます：\n",
        "* 「高速に推論する」など、従来の一般的な推論最適化と「Inference Time Scaling」を混同した誤った解釈が継続\n",
        "* 講義内容を参照しているものの、概念の本質を正確に捉えられていない\n",
        "\n",
        "### 問題分析\n",
        "以下の要因が考えられます：\n",
        "1. **ドキュメント品質の問題**: 音声認識による文字起こしの精度不足\n",
        "2. **検索精度の課題**: 単純な文単位の分割では文脈が失われ、関連性の高いドキュメント片を適切に取得できていない可能性\n",
        "\n",
        "### 書き起こしテキストの品質改善\n",
        "\n",
        "日本語の音声認識（speech2text）モデルは精度に課題があることが知られています。以下に「LLMにおけるInference Time Scalingとは？」に関連する講義内容の書き起こしテキストを比較します："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q83QyfAIphk6"
      },
      "source": [
        "### 講義中の該当発言 (LLM講座Day4後半から抜粋)\n",
        "\n",
        "\n",
        "<修正前>\n",
        "---\n",
        "\n",
        "講義に戻ります。ちょっと練習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドって話で**生のGENIACLM**の話をして終わろうと思いですねちょっとモチベーションから話すと、ちょっと頭で考えてみてほしいとか見れば一瞬で思うとんですけどバナナの色は何ですかって言われたときと、今日の講義聞いた上で、**ゲームソフトの問題は何だと思いますか**って聞かれたとき、多分あの考えることが違うと思うんですね。**羽の色なんですか**っていうと一瞬黄色ですねもしかしたら緑かもしれないけどぐらいですかね物によるかなみたいなおもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも**スケール足の問題なんだと思いますか**って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだからどうだろうこの辺が問題かなみたいな考えとやっぱ思考としては違うってことは何となく思うかなと思います。なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。**スケールって言ってる7Gのスケール**って言ってるのはこういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、あの考えなきゃいけない問題に対しては、考える時間を、に計算式を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。二つで、ちょっと順番が前後しますけどこれの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近**大湾**と呼ばれる論文が論文じゃないか、モデルが**オペル**から出ましたプレビューとして出てますけどこの法案で注目されていますこれあの**論文にROMってかブログ**にあるとイエスって右側が訓練時の計算資源をスケールさせたときに、初めて何かロジックのベンチマークがあるんですけどこれをがどうなったかで何となくスケールしてると右側がテストTimeコンピュートっていうふうに書いてると思うんすけど、**水温時**に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだんコース計算式を増やしていったときに、性能がどう変わるかっていうのでこれもスケールしていってるということがわかると思います。こういった形で、要は考える時間をどうやら推論時に使うと計算資源を推論使うのはいいことがありそうだということがわかります。\n",
        "\n",
        "\n",
        "<修正後>\n",
        "---\n",
        "\n",
        "\n",
        "講義に戻ります。ちょっと演習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドってことで**「推論時のスケーリング」**についての話をして終わろうと思います。モチベーションから話すと、ちょっと頭で考えてみてもらえれば一瞬でわかると思うとんですけど、「バナナの色は何ですかって言われたとき」と、今日の講義聞いた上で、**「スケール則の問題は何だと思いますか」**って聞かれたとき、多分あの考えることが違うと思うんですね。\n",
        "**「バナナの色なんですか」**っていうと黄色ですね。もしかしたら緑かもしれないけど、物によるかなみたいな、おもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも**「スケール則の問題なんだと思いますか」**って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだから「どうだろう」「この辺が問題かな」みたいな考えとはやっぱ思考としては違うってことは何となく思うかなと思います。\n",
        "なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。**推論時のスケールって言ってるのは**こういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、深く考えなきゃいけない問題に対しては、考える時間に計算資源を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。\n",
        "これの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近**o1**と呼ばれるモデルが**OpenAI**から出ました。プレビューとして出てますけどこのo1で注目されています。これあのo1の**論文ってかブログ**にある図で、左側が訓練時の計算資源をスケールさせたときに、AIMEというロジックのベンチマークがあるんですけど、accuracyがどうなったかというと、何となくスケールしてる。右側がtest-time computeっていうふうに書いてると思うんすけど、**推論時**に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います。\n",
        "こういった形で、要は考える時間をどうやら推論時に使うと、つまり計算資源を推論時に使うのはいいことがありそうだということがわかります。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrp81WzyhYc"
      },
      "source": [
        "---\n",
        "### 文字起こしの誤り\n",
        "\n",
        "上記の比較からわかるように、音声認識による書き起こしには重大な誤りが多数含まれています：\n",
        "* 「スケール則の問題」→「ゲームソフトの問題」\n",
        "* 「o1」→「大湾」\n",
        "といった明らかに文脈に合わない単語変換が発生しています。\n",
        "\n",
        "`LLM2024_day4_raw.txt`の中には、このような誤変換が多数見られます。これらの誤りはRAG性能に直接影響し、モデルの回答精度を低下させる要因となります。\n",
        "\n",
        "したがって、**ドキュメント品質の改善**を行い、RAG性能の向上を図ります。\n",
        "\n",
        "## 講義内容をソースとして活用：改善版RAG実装\n",
        "\n",
        "* **ドキュメント処理**: \n",
        "  - speech2textによる書き起こしテキストを人手で丁寧に修正\n",
        "  - 専門用語（Inference Time Scaling、GPT-o1など）の正確な表記を確保\n",
        "  - 文脈の流れを維持しつつ、文法的に正確な日本語に修正\n",
        "\n",
        "* **検索手法**: \n",
        "  - 引き続き「。」（句点）で区切られた文単位でテキストを分割\n",
        "  - 文単位の検索により、モデルの入力トークン制限内で関連情報を最大化\n",
        "\n",
        "この改善により、モデルが正確な情報に基づいて「Inference Time Scaling」の概念を理解し、適切な回答を生成することが期待されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNjIC4RnzkNW"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/lecture-ai-engineering/day3/data/LLM2024_day4.txt\", \"r\") as f:\n",
        "  raw_writedown = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f53OojeTzkNW"
      },
      "outputs": [],
      "source": [
        "# ドキュメントを用意する。\n",
        "documents = [text.strip() for text in raw_writedown.split(\"。\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[310])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlduigQ3OfoN"
      },
      "outputs": [],
      "source": [
        "# Retrievalの実行\n",
        "question = \"LLMにおけるInference Time Scalingとは？\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "# 各ドキュメントの類似度スコア\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNsGUsnlOoMm"
      },
      "outputs": [],
      "source": [
        "topk = 5\n",
        "for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "  print(documents[index], \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoOCvFW4ltcA"
      },
      "outputs": [],
      "source": [
        " #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n",
        "references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FbzMLfTtWxx"
      },
      "outputs": [],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLe0IJPeH97d"
      },
      "source": [
        "## 結果 (修正テキストによるRAG)\n",
        "\n",
        "書き起こしテキストの品質改善により、モデルの回答に部分的な向上が見られました：\n",
        "\n",
        "### 改善点\n",
        "* 「推論時（Inference）に計算資源をスケーリングすることで、モデルがより賢くなり、性能が向上すること」という概念を正確に捉えるようになった\n",
        "\n",
        "### 問題点\n",
        "* 「Inference Time Scalingは、TransformerやLSTMなどのモデルにおいて、パラメータ数を増やすのではなく、推論時計算資源をスケーリングすることで、性能が向上すること...」という記述は講義内容と矛盾している\n",
        "\n",
        "### 問題分析\n",
        "\n",
        "モデルが誤った回答を生成する主要因として、**文脈の欠如**が考えられます：\n",
        "* 「。」で区切られた短い文単位での検索では、各文の発言背景や関連性が失われる\n",
        "* 単独の文から情報を抽出するため、講師の全体的な主張や議論の流れを把握できない\n",
        "* 結果として、正しい個別の文でも、その解釈に必要な背景情報が欠如し、誤った文脈で理解される\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 3. 文脈を考慮したチャンク化の導入\n",
        "\n",
        "検索結果の品質向上のため、以下の改善を実施します：\n",
        "\n",
        "* **前後文脈を含むチャンク化**:\n",
        "  - 検索でマッチした文だけでなく、その前後の複数文も含めてチャンクとして取得\n",
        "  - 具体的には、マッチした文を中心に前2文、後2文を含む計5文程度のチャンクを構成\n",
        "  - この「文脈ウィンドウ」により、発言の背景情報や議論の流れが保持される\n",
        "\n",
        "* **期待される効果**:\n",
        "  - 講師の主張とその根拠の関係性を正確に把握できる\n",
        "  - 概念の定義とその適用範囲を正しく理解できる\n",
        "\n",
        "この改善により、モデルが講義内容の本質をより正確に理解し、一貫性のある事実に基づいた回答を生成することが期待されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94uovDFrVOTJ"
      },
      "outputs": [],
      "source": [
        "# 前後それぞれ2つずつの文章を一つのドキュメントに追加する。（要は5つの文章集合になる)\n",
        "references = \"\\n\".join([\"* \" + \"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]])\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAzYsxVWVdMS"
      },
      "outputs": [],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD3R54G1WX8B"
      },
      "source": [
        "## 結果 (文脈付きチャンク化によるRAG)\n",
        "\n",
        "文脈を含むチャンク化により、モデルの回答の方向性に明確な改善が見られました：\n",
        "\n",
        "### 改善点\n",
        "* 「推論時の計算をスケールさせる」という概念を据えて回答\n",
        "* Inference Time Scalingの基本原理についての理解が向上\n",
        "\n",
        "### 残存する問題点\n",
        "* 質問と関連性の低い情報（ノイズ）が混入する\n",
        "\n",
        "### 問題分析\n",
        "\n",
        "文脈付きチャンク化によるアプローチで新たに発生した課題：\n",
        "\n",
        "1. **情報過多の問題**:\n",
        "   * ドキュメント量の増加により、モデルに提供される情報総量が大幅に増加\n",
        "   * 関連情報と非関連情報が混在し、ノイズと重要情報の区別が困難に\n",
        "\n",
        "2. **情報選択の複雑化**:\n",
        "   * モデルは単に回答を生成するだけでなく、提供された多様な情報源から関連性の高い情報を選別する作業も担うことになった\n",
        "   * この二重タスクにより回答生成の難易度が上昇\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Rerankによる情報品質の向上\n",
        "\n",
        "検索精度をさらに向上させるため、二段階の検索プロセスを導入します：\n",
        "\n",
        "* **Rerank手法の導入**:\n",
        "  - 第一段階: 従来通り基本的な検索アルゴリズムでtop-k個のドキュメントチャンクを取得\n",
        "  - 第二段階: 取得したチャンクに対してLLMを活用した高度な関連性評価を実施\n",
        "  - LLMに「このドキュメントは質問『LLMにおけるInference Time Scalingとは？』に対して本当に関連性が高いか」を判断させる\n",
        "  - 関連性スコアに基づいてランク付けし、真に関連性の高いチャンクのみを選出\n",
        "\n",
        "* **期待される効果**:\n",
        "  - 質の高い情報に焦点を絞ることで、ノイズとなる情報を大幅に削減\n",
        "  - 文脈を保ちながらも、関連性の高い情報のみをモデルに提供\n",
        "  - モデルのタスクを「多量の情報から選別して回答」から「厳選された情報に基づいて回答」へと単純化\n",
        "\n",
        "この高度な情報フィルタリングにより、Inference Time Scalingに関する正確で一貫性のある回答生成が期待されます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HfzJ5EpXGtj"
      },
      "outputs": [],
      "source": [
        " #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n",
        "references = []\n",
        "for ref in [\"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]]:\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": f\"与えられた[参考資料]が[質問]に直接関連しているかを、'yes''no'で答えること。[参考資料]\\n{ref}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "  ]\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      # max_new_tokens=128,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=False,\n",
        "      # temperature=0.6, # If do_sample=True\n",
        "      # top_p=0.9,  # If do_sample=True\n",
        "  )\n",
        "\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "  response = tokenizer.decode(response, skip_special_tokens=True)\n",
        "  print(\"\\n\\n対象となるドキュメント:\\n\", ref.replace(\"。\", \"。\\n\"))\n",
        "  print(\"\\n関連しているかどうか: \", response)\n",
        "\n",
        "  if \"yes\" in response.lower():\n",
        "    references.append(ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLietDaD5I3h"
      },
      "outputs": [],
      "source": [
        "print(len(references))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs74h4ADXj99"
      },
      "source": [
        "上記より、上位4件のみが関連しているとわかったので、これらだけをモデルに渡すこととする。（生成内容が確立的なので、4件でない可能性もあります）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu9wBykZXxja"
      },
      "outputs": [],
      "source": [
        " #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"与えられる資料を参考にして回答すること。[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5kHntvSXxjb"
      },
      "outputs": [],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elqD2gJt5RCo"
      },
      "source": [
        "## 結果 (Rerank導入後)\n",
        "\n",
        "Rerankの導入により、回答品質に改善が見られました：\n",
        "\n",
        "### 達成された成果\n",
        "* Inference Time Scalingに関する正確な情報を含んだ回答の生成\n",
        "* 無関係な情報やノイズの排除\n",
        "* 講義内容を反映した説明の実現 🎉\n",
        "\n",
        "この結果から、RAGパイプラインにおける情報の質と関連性の重要性であり、検索で取得した情報を単に増やすだけでなく、その情報の関連性を精査する方法を学ぶことができました。\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 5. さらなる改善案: 意味的チャンク化\n",
        "\n",
        "文単位での分割と前後文脈の追加という現在のアプローチをさらに発展させる手法として、**意味的なチャンク化**が考えられます：\n",
        "\n",
        "* **意味的チャンク（段落）単位での分割**:\n",
        "  - 単純な文の区切りではなく、意味的なまとまり（トピック、議論、例示など）に基づいてテキストを分割\n",
        "  - 人間の主観に基づく意味的な段落分けを活用\n",
        "  - 各チャンクが「一つの完結した考え」を表現するようにする\n",
        "\n",
        "* **期待される効果**:\n",
        "  - より自然な文脈理解が可能に（人間の思考や会話の流れに近い）\n",
        "  - トピックの開始から結論までの流れを維持できる\n",
        "  - 概念間の関係性や比較が同一チャンク内に含まれ、より深い理解につなげる\n",
        "\n",
        "* **検証方法**:\n",
        "  - 人間が主観的に意味でグループ化したチャンクセットを用意\n",
        "  - 同じRerank手法を適用し、文単位チャンクとの性能差を比較\n",
        "  - 回答の正確性、一貫性、網羅性を評価指標として使用\n",
        "\n",
        "この意味的チャンク化手法は、特に講義のような構造化された発話においては、より自然で効果的な情報検索と理解を可能にすると予想されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU_ttvcNKayo"
      },
      "source": [
        "**注意事項**\n",
        "\n",
        "**ここから先のセルを実行した場合、GPUメモリ不足になる可能性が高いです。**\n",
        "\n",
        "\n",
        "このノートブックでは、GPUはT4を使用しています。\n",
        "Colab Pro等を契約し、L4などのよりGPUメモリの大きいものを使用するか、モデルやその設定等を変更するなどの工夫が必要になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DNOyPNXAtl3"
      },
      "outputs": [],
      "source": [
        "# 本来は段落をそのままdocumentsに入れずに一定のサイズに分割した方が良いでしょうが、簡単のために段落をそのまま入れてしまいます。\n",
        "documents = [text.replace(\"\\n\", \" \").strip() for text in raw_writedown.split(\"\\n\\n\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF6wc10RAxuc"
      },
      "outputs": [],
      "source": [
        "question = \"LLMにおけるInference Time Scalingとは？\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-FKkAcTA-Sx"
      },
      "outputs": [],
      "source": [
        "# 簡単のためにtop2でやります。結果を見てもらえれば問題なく関連する項目のみ取得できているのが分かるかと思います。\n",
        "topk = 2\n",
        "for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "  print(documents[index], \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtC-wsj4BGwn"
      },
      "outputs": [],
      "source": [
        "reference = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"[参考資料]\\n{reference}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device)\n",
        "\n",
        "terminators = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    # max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=False,\n",
        "    # temperature=0.6, # If do_sample=True\n",
        "    # top_p=0.9,  # If do_sample=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c27VI95SCzV5"
      },
      "outputs": [],
      "source": [
        "response = outputs[0][input_ids.shape[-1]:]\n",
        "print(tokenizer.decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJVL3u6lCc8k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
