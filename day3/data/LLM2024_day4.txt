早速内容ですけど、目的はタイトルの通りですけど言語モデルスケール則について学ぶってことで、大規模言語モデルっていうふうに呼ばれてますけど、ちょっとスケール則の話とか初回も少ししましたけど、これだけ大きくなっている一つの理由になってますのでそのスケール則ってどういうものなのかとかそれがなぜ重要なのかっていうところ、説明できるようなってもらうというところと、スケール則ってどうやって求めるんでしたっけというところを説明実装できるようになるところについて中心的に話していければと思ってます。
あのスケールするっていうところではタイトルの通りなんですけど、ちょっとこれスケーリングPretraining回ってなってるんですけれども、ちょっと最近はですね、このPretrainingだけではなくて、推論をスケールさせるというような話も出てきてましてせっかくなのでその最近の話題ということです推論時のスケーリングことで、ちょっとタイトル詐欺が入ってるんですけどPretrainingだけじゃない、スケーリングも扱うということで、ちょっと若干あのタイトル詐欺なんですけども、あの最近の話題ということで推論時のスケジュールについても話していきたいなと思っています。
演習では2つ目のポイントに近いですけどスケール則を実際に求めるというところでそのコードを実装できるようになってもらうということを目的としています。では早速ですけど中身に入っていきたいと思います。ちなみ余談なんですけどこれ実はDay4が去年から結構変わってまして、去年Day4だったものをDay4とDay4とDay8に分けてるんですけども、分けたはずのDay4がなぜか90枚スライドがあるという状況でしてちょっと若干早口になって申し訳ないんすけど少し資料を補足としてやっていただきながら自学できるようにしてあると思いますんで、何かわかんないとこあったら資料読んでもらえると思います。


目標の通りですけれども大きく四つの話をしたいと思ってます。最初がスケール則とは何かそもそもどういうものだったかっていうところをご説明したいと思ってます。何かっていうのを先に学んだ後に、なんでこれ学ばなきゃいけないのか、これ知ってると何のいいことがあるのっていう意味でスケール則どういうふうに使ってるのか、他にどういうふうに使われてるのかってとこについて、2個目でお話したいと思います。
三つ目がそれをどうやってまとめるとか、演習に関係するところですね。最後に新しいトレンドっていうことでちょっと時間の関係で若干駆け足なるかもしれないけど、推論時のフィーリングっていう最近のo1とかも出てますけど、その辺で使われている技術について、正確には使われているとされている技術についてちょっとお話したいなと思っています。

まずスケール則とは何かっていうところですね。
あの最初にも言った通りですけれど、スケールっていうのは、その大規模言語モデルを支える一つの大きな要素になってます。言語モデルっていうところがDay3でやりましたけれども、最近はTrasnformerになっていて、Transformerの中にもいろいろ種類があるということで発展的課題がまたコミュニティの方であるってありましたけども、基本的にはこの言語モデルを作る技術について言語ってどういうものですかっていうのをDay3でパターンだかなと思います。
Day4ではこの大規模っていう側にフォーカスを当てまして、どうスケールさせるのかとか、なんでスケールさせることが有効なのかそれを支えるあのスケール則っていうものについてお話したいと思います。講義全体の位置づけも少しおさらいしておきますとます今日までが行った事前学習と呼ばれるような枠組みの話になります。
また非常に大規模に学習データで訓練するっていうプロセスです。そっから次回以降はFine-Tuningってことで少しまた違う話をしますので一旦今日までがあの話の区切りだと思ってもらえればいいかなと思います。


中身に入っていきます。こちらのこのスライド初回にもお見せした通りですけれども、大規模言語モデルと呼ばれるものが進展していますということですあのこのサーベイ論文紹介にも紹介した通りですけど2023年の間に13回ぐらい更新されるぐらいモデルがどんどん出てると、2024年だって今年になってもいろいろなモデルが出たり、なんか巷で試されて実はこれは良くないんじゃないかと言われたりいろいろしているというような状況になっています。
それからこれも最初紹介に示した通りですけども、基本的にこの裏側にはスケール化というのが進んできています。元々2018年ぐらいが117Billonだったのが、2020年には175Billonと1000倍ぐらいです企画なっていてGPT-4は1Billon以上だというふうに言われています。
このスケールっていうのがやっぱ重要な要素になってきています。

こういうスケールしてるわけですけど、何でこんなにスケールしてるのかと、あるいはこのスケールするってどういうことなのかっていうのを今日は話していきたいと思います。
特にですね背景にあるのが一番よく出てくるのがスケール則と呼ばれる経験則でして、最初の方ではこの二つの論文を中心に説明したいと思います。一つがOpenAIが2020年に出した論文でして、Scaling Laws for Neural Language Modelというような論文になっています。
これちなみにGPT3が2020年、このスケーリング則がでたあの約半年後ぐらいに出てまして、GPT4の中でもこのスケーリング則の話とか出てくるようなものになってまして基本的にこのGPT3が開発された背景に、こういうスケール則の発見というのがあったというふうに言われています。
もう一つがこれを少し拡張した形の研究としてTraining Compute-Optimal Language Modelというような研究があります。これ、当時のDeepMind今GoogleDeepMindですけど、が出した論文でしてこのスケール則の考え方を使って、どういうふうなデータと、どういうふうなモデルサイズを用意すれば良いモデルが作れるのかっていうのを検証した論文になってます。
この辺り中心にいろいろな研究をされながら今日はご説明できればと思ってます。

早速スケール則とは何かっていうところについてこのスライド自体は初回に多分お見せしたかなと思いますけれども、どういうものかっていうのをおさらいすると、基本的な計算資源とデータサイズ厳密にtoken数ですね、学習に使っているtokenの数とモデルのパラメータ数っていうのと誤差、その学習に使ってる誤差ですね。
正確にいうとテストデータにおける誤差の間にある経験則が存在するというようなことを指したあの言葉になってます。どういう経験則かというと、例えばわかりやすいのが右側のパラメータなのでパラメータの方に注目すると、この青い点を打つっていうのが、実際にどれぐらいのパラメータを使ったときに、どれぐらいの損失だったかっていうのを表してますけれども、これをプロットすると、両対数グラフ要は10のN乗みたいな形で書いたときですね、こういった綺麗な直線になるというような法則になってますね。
これがパラメータだけじゃなくて、データサイズだったり、計算量に対しても、横軸をそれぞれ変えてやったときに、同じように、こういった対数空間上での線形のスケールが成立しますよというような経験則になってます。
何か細かいことを一応書いてますけど一応これ要は点が実測値になってまして、この実線みたいなのが、予測値に該当してます。これあとは他の2変数は十分に大きいと仮定したときのあの実験結果になってます。


一つ一つ見ていきますと今見たのと同じ順の話ですけどパラメータ数についての図をピックアップしたのがこの図にあります。さっきと同じで10の5乗,10の7乗,10の9乗って形で横軸が対数の対数グラフになってまして、例えばCross-Entropyで対数化された値になっています。
これを見ていくと非常に綺麗に、ちょっとガタガタ若干してますけど、巨視的に見ると、大体この黒いフィッティングした線と同じになっています。
それから次の図がデータセットのところについてピックアップしたものですけれど、さっき言った通りデータサイズって言ってるのは基本的なtokenの数だと思ってください。10の8乗tokenとか10の9乗tokenっていう単位これについてもパラメータと同様に点の値をプロットしていって、その間をフィッティングしてると非常に綺麗な関係が見えることがわかるかと思います。
最後が計算資源でして計算資源だけちょっとわかりにくいんですけどまず横軸がこのPeta FLOPs Daysというのになってます。Peta FLOPs Daysっていうのが何かっていうのを説明しますと、一応補足というか知ってる人は聞き流してもらえばと思いますけど、計算ってのはどれぐらいの浮動小数点演算を行っているかを表してる単位だと思ってください。


浮動小数点演算っていうのはコンピュータしてる人だと分かると思いますけど、パラメータの足し算とか掛け算に該当してまして、細かい話するとディープラーニングの場合だと普通FP32とか1Byteですね、の計算が使われるので1Byte同士の足し算みたいのが1演算、で掛け算も1演算という形で表せるになってます。
これなんか右下に巨大な巨大な2階建てMLPって書いてますけど、このWをかけるみたいな、これが掛け算に相当しますし、例えばバイアスを足すみたいなもバイアスを足すということで基本的にニューラルネットワークの計算ってのはこの浮動小数点のパラメータを足したりかけたりするというので表すことからできることがわかると思います。
「紛らわしいか」っていうのちょっといっぱい無視してもらえばいいと思うんすけど、この合計の浮動小数点演算の数を表すものとして、FLOPsというのが使われています。
さっき横軸がこの要は浮動小数点演算の合計の数になっているというふうに理解いただければと思います。ちなみに「紛らわしい」て書いたのが大文字の「S」のFLOPSっていうのもありましてこれがFloating Points Operation per Secondsっていうものになってまして、1秒間当たりどれぐらい計算できるかっていうのものになってます。
これあのGPUの仕様書とか見ると、こういう感じでFP64とかFP32で例えばこれ19.5TFLOPSみたいな書いてあるんすけどこれは要は、1秒間当たり何回計算できるか、基本的にはハードウェアの計算能力を表す単位だと思ってもらえばと思います。ちょっと戻りますけど、ちっちゃい小文字のsは全体でどれだけ浮動小数点演算が必要かというものを表すもの。と若干違うということでご理解いただければ。要はあのスケール則を見たときに別に演算能力が上がってるわけではなくて合計で必要な計算量が上がってるというふうに理解いただければいい。


あのスケール則の方に戻りますと、パラメーターの話とパラメータとデータサイズ(token数)の話と若干違うとこがありまして、点じゃなくてこういうなんか曲線が打たれてます。
あのこの曲線っていうのが、あのこの図では、あの異なるモデルサイズで学習したときの学習曲線をイメージして意味しています。要は横にいくほどだんだん計算量がかかってくるので、学習したときのこの1個1個の学習曲線がこの青いなんか薄い線に対応してると思ってください。
これ例えばこの特定のパラメータで学習したときの曲線がこの1個の中に該当するということで、あの別のものが違う線、でさらに違うものが、これ例えばN''が一番大きいので計算量が掛かる。N'が真ん中ぐらいでこれぐらいの計算。Nがこれぐらいで計算するっていうような意味合いです。このようにを変えてやったときにこのオレンジの線っていうのはこの一番ベストな条件、この計算量を取ったときに一番いい条件で学習した時の値っていうのがこのオレンジの点線でフィッティングされてるというふうに理解いただければ。これモデルサイズが小さいときは最初早く学習が進むわけですね計算量が少なくても性能が出るので計算量が少ない前提だと、モデルサイズがちっちゃい方がいい。
逆にモデルサイズが大きいと、なかなか学習が進まないんですけど、最終的な性能が良くなるということでこれが最適なレートみたいのが存在するということです。ちなみにこれは読み方というか使い方に近いですけれど横軸テストロスをある値にしたときに、どういうものが良いモデルかっていうのは、この横をピーっといけば大体わかって、これちょっとパラメータ数がいくつかってのはわからないですけどこれN'ぐらいがいいってことで、めちゃめちゃ大きければいいというわけでも小さければいいわけではないことがわかりますし、自分が計算資源どれぐらい持ってるとき、何かパソコンを何か100台ぐらい持ってますとか何か自分が使える計算量がわかってるときにも、これをぴーっと線をひいてると大体これぐらいのパラメータを取ればいいんだなってことでそれがわかるということになります。


これがあのスケール則の詳細ですね。コンピュートのところ以外は割とわかりやすいですけれど、こういった関係が成立するってのがスケール則と呼ばれているものです。若干細かい補足なんですけれどというかあの、さっきからこの謎の値が書いてあったと思うんすけど一応詳細に見ておくと補足なんで理解した人だけでいいですけれどこれよく見るとこういうべき乗の関係になっていることがあります。
このLっていうのがロスですね、最適化したりとか小さくしたいもので、ちょっとこれ若干順番が違うんですがX_cっていうのが、この例えば2.3掛け10の8乗みたいな、これがコンピュートの場合ですけど、係数に該当していて、アルファがこのマイナス0.05みたいな対応するということでこれ今コンピュートの図だけ出してますけど、他のものも同じような形をしています。
これ何で両対数にすると線形になるのかっていうことですけどこれログ取れば明らかで、どっちもログ取るとこういう感じになるので、logX_cがこれ横軸なんで線型の普通の一次の式で、αが傾きの線みたいな感じになることがわかります。αlogX_cが接線みたいなもんですね。そういった形になるので、このスケール則というのはこういうこのべき乗の形で書けるのでスケール則というふうに言われています。


ここまでスケール則の話をしてきましたけど、あのそうですねモデルサイズがまだ小さい状況でのスケール則っていうのを検証してましたけど、さっき半年後ぐらいにGPT3が出ましたよって言いましたけど、GPT3でもこのスケール則ってのを検証しているってことが報告されてます。
さっきのあの学習のときと同じような図があの論文には載っていて、これ色がそれぞれパラメータの違いに該当しています。黄色が一番大きくて青が一番ちっちゃい、紫かな、ちっちゃいモデルになってます。先行研究、さっきの今まで話してたものより2桁オーダーが大きいモデルにおいても、これ厳密に言うとまだ収束してないように見えるんですけど、おおむねスケール則が成立してるっていうことがGPT3の論文だと報告されています。


あのスケール則についてここまで他の多分一番有名なのが先ほどのから話しているScaling Laws for Neural Language Modelというものなんですけれど、実はこのスケーリングっていうの自体、スケーリング則、スケール則が成立するってこと自体は、もうちょっと前から知られていたというふうに言われています。
ちょっと僕ももしかしたらもっと昔からあるかもしれないんで僕が知ってる限りですが、少なくとも2017年の論文では検証されているということが言われてます。このDeep Learning Scaling is Predictable Empiricallyという論文があります。
ちなみにこれ今日話さないんですけど理論的にどういう条件だとスケール則が成立するのかっていうのを議論していたりもするので、もし興味ある人はこの論文化後か、あとはスタンフォードのレクチャーでこの辺触れられていたのでもし興味ある人は見てみると良いと思います。
少しだけ内容に触れますと、この論文では当然2017年なんでこれTrasnformerがあの出たか出ないかぐらいの時期なのであのTrasnformerのスケール則っていうのはやってなくて、LSTMだったり、RHNというか、リカレントハイウェイネットワークっていうちょっと何ていうか、LSTMの亜種みたいなものをですね、時系列のResidualネットワークみたいなことを使った研究になってます。
これ見てもらうと横軸がトレーニングデータセットのサイズ、縦軸がロスですね、ログスケールのロスになってまして、さっき見たのと同じような横線の図が広がっているというふうに描かれていることがわかるかと思います。
厳密に言語モデルじゃなくてMachine Translationの結果だったということです。あとは異なるとして対象モデルが違うってことでTrasnformerじゃないモデルを使ってますよとか規模も全然違うものですけれども、初期的にはこういった結果も知られていました。
それをスケールアップさせたのが先ほどのOpenAIの研究だというふうに説明できるかなと思います。

それから元のOpenAIの論文に戻りますと、今言ったようなLSTMの比較みたいなLSTMにおけるスケール則みたいなことも、この論文でも検証されていまして、左側がモデル構造が違うんですね。
Trasnformerの場合はスケール則が、パラメータ数が横軸になってますけどこういうふうになると、LSTMの場合には1層2層4層みたいにそれぞれスケール則を解くとこんなふうになりますよということで、Trasnformer以外のスケール則っていうのもあの検証をされている。深さについても検証してまして、これも元のモデルが何だったかちょっと忘れちゃったけど、確かLSTMだったような気がしますけど、層を変えたときにどういうふうな変化するかっていうのをこういった形でプロットするようなことがされてます。
ポイントはTrasnformer以外でも別にあのスケールするっていうのは成立概念だということです。なんでこんなTrasnformerだけ注目されてるのかってのは後で話します。

それからこれは補足に近いですけど、Trasnformerの中でよく知ってる人だと最近はMixture of Expertって呼ばれるモデルがよく使われているということを知ってる人も多いと思いますけど、このMixture of Expertにおいてもスケール則が存在するってことは言われています。
多分他にもあると思うけど代表的な論文で上が初期で下が最近で、なんかちょっとやり方でスケール則を検証してるものが下になります。例えばこんな感じで点線がうんTrasnformerで実線がこのMixture of Expertと呼ばれるモデルをスケールさせたときにどういう変化をするかというものを見ているものですけど、こういった形でMixture of Expertにおいても、スケール則が成立するとかなり綺麗な関係が成立するということが言われています。
このMoE自体はちょっと今回の対象ではないのでMoEがどういうモデルかっていうのはDay8を楽しみにしておいてもらえばと思います。ここでポイントはrasnformer以外でも別にこのスケール則というのは、あの成立しているということです。


それから基本的に今までの話は言語modelingなのでnextのtokenを予測するときの損失について縦軸としていましたけれど、それ以外にも他のドメインでもこのスケール則っていうのが整理するよっていうことも報告されています。
これもOpenAIが出してた論文で、これ左がイメージのモデリングでテキストとイメージとかビデオとか数理計算とかimage2textとか、こういったものが他のドメインでも成立するよということも言われています。


それからもう一つの論文でChinChilla Training Compute Optimalっていうものがあります。ここまでが1変数を制御している、横軸1変数にして、他の二つの変数については際限がないというか無限にあるという前提での経験則でしたけど、2変数を制限した場合の経験則っていうのも知られています。
それが有名なのがこのTraining Compute Optimal Large Language Modelというものでしてそれが左側の実験結果、右側がPaLMと呼ばれるGoogleが出したモデルのバージョン2のPaLM2というモデルがありますけど、そのときにホワイトペーパーから取ってきたやつになってます。
どっちも同じような図になってますけれど、これていうのはそれぞれの、例えば左側を見ると、6E-18とか1E-19と書いてますけど、この6E-18とかが使える計算量を意味してると思ってください。なので例えば左上のこの薄いやつ、薄い緑のやつは、それぞれ6E-18の計算量を使って、何かを変化させるというものです。
その何かっていうのがこのパラメータになってまして、例えばこのなんかどこだろうこの線を見ると、いくつなんだろうわかんない100ミリオンとか、この線を見ると300ミリオンとかぐらいをパラメータに割り当てた場合、ということになってます。
当然パラメータを増やしたり減らしたりするだけだと、計算量っていうのは変わってしまうので、この場合は学習時間は固定していてtoken数を変更させているっていうなものになってます。なんでこの丸はそれぞれ学習時間とtoken数が変化しているというふうに思ってください。
これをいろんな計算時間に対して見ていくと、どれも概ねこのUshapeというかの下側に凸な形になっていることがわかると思います。これ要は最適な値がありそうだということですね。ある計算量を考えたときに、めちゃめちゃ巨大なモデルを使うでもなく、めちゃめちゃ小さいモデルを使うのではなく、最適な値がありそうだということがわかると思います。
右側も同じような結果ですね、あの形は微妙に違いますけど大体同じようなことわかると思います。これがあのChinChillaの論文でして、これの使い方もまた後で話します。

ここまでがあのスケール則とは何かのまとめでして、スケール則っていうのは毎回おさらいすると、計算資源とデータセット、パラメータと誤差の間にはこういった経験則がそうですよ。
こういうべき乗則で書けますよっていうのがスケール則でした。両対数グラフで線形なるのは両対数をとってやるとわかるということも説明しました。それから一番有名なのはTrasnformerで本当のスケール則ですけど、それ以外のモデルでも成立しますし、言語以外のタスクでもスケール則ってのは確認されていますという話をしました。
それから1変数を制御するのではなくて複数の変数を制御するような経験則も知られていて、有名なのではChinChillaと呼ばれる経験則があります。ChinChilla論文って呼ぶ方が正しいかもしれない。

ここまでで計測スケール則の話をしたんですけどChinChillaのところで少しパラメータ数とデータセットサイズ、それぞれいじって計算量を固定しますよって話をしたのでちょっとその補足をしておきたいと思います。
これよく出てくる式、あの経験則の近似式なんですけど、学習に必要な計算量ってどうやって計算してるんですかっていう話があると思います。これは「6*パラメータ数*token数」っていうふうに、あの計算されることが多いです。
これ例えばGPTの場合だと175BillonがNに相当します。0.3テラtokenがDに相当する、それに6を掛けたもので3.14*E+23FLOPSというふうに計算できます。
これ近似って書いてある通りこの6っていうのは近似なんですけど、これは概ねなんでこの6かっていうと、これ興味ある人だけでいいですけど、1層当たり、1パラメータあたりのMLP層における計算が6回あるということに起因しています。
これアニメーションついてるんすけどアニメーションめっちゃ早いんですけど、基本的には1回MLPを計算するときに、フォワード方向で掛けて足すので2回、Backwardではそれが2回行われる、Backwardで入ってきたときに掛けて足すのと、自分が外に出す値を計算するときに掛けて足すすっていうので、4回あるので、大体1パラメーター当たり6回計算するというのでこの6という数字が使われています。
これなんかよく謎の式として出てくるので少し補足しておきます。ちなみにこの近似って書いてある通りこれはあの雑な近似ではあります。というか下に「MLP層における」って書いてある通りなんですけどMLP以外の層も当然あるのでそれ以外の層では違う値が実際には厳密には必要になります。
ただこれ系列長が短い場合だと、ちょっと僕も正確には詳しくないですが、MLPの計算量の方がAttentionより低い、一方で系列が長いとAttentionの計算で大きくなってしまうんですけど、そうじゃない場合基本的にMLPの計算力のがあの膨大なので大体無視できるでしょってことでこういった式が使われているそうです。
最近はどんどんtoken数伸びてきているので、若干無視できなくなっている傾向があるかなと思います。あと正確な式の例っていうのもありましてこの実装とかだとこういう何かちょっと例えばエンベディングでは2を掛ける。
Attentionでは2×3×sequence_length×d_model×(key_size×num_head)みたいな、こういった感じで厳密にこういうふうに計算することができます。ただスケール則をやるときにはそのモデル同士の比較ができればいいので、多分そんなに気にしないで適当にこのFLOPSっていうのを計算してるんだと思います。
はい。ちょっと補足でした。

ちなみに補足がてらよく使うので見にくい図というか、考えてみてもらえばということでちょっと答え書いてないんですけど、さっきFLOPS大文字のSの概念と、あの計算量の話GPT3だと例えばこの3.14×10の23乗ありますよって話をしましたけど、これを使うと大体GPT3の学習にどれぐらい計算時間が必要か、A100が1000基あるといったときにどれぐらい必要かっていうのを見積もることができます。
このFLOPSとかよくこういう計算にも使うので、興味ある人はちょっと計算してみてもらえばと思います。はい。今やらなくて大丈夫、割ればいいだけです。ちなみに言語モデル開発しようと思うと、めちゃめちゃよく使う意識でして、WebLab_10Billonとか開発したときは裏でこういう式とかめちゃめちゃ飛び交ってました。


これは裏話でここまでスケールとは何かについて話しましたがこれは何かっていうのは一旦理解できたと。そういうべき乗の関係にあって両対数すると線形になるんですね。1変数じゃなくて2変数の経験則ってのもあるんですけど、そういったことは理解できたと思いますけれど、それどうやって使うんですかっていう話を次していきたいと思います。


これ同じですねこのべき場の関係が成立しますよというような図です。これがスケール則でした。これどうやって使うんですかっていう説明をするにあたってちょっと使われてる例をいくつかピックアップしました。これGPT-4のテクニカルレポートから取ってきた図ですけれどこの緑が実際のGPT4のパラメータ数だというふうに言われていますこれが1になってます。
これに対してGPT4では、それより1000分の1ぐらいちっちゃいモデルでのスケール則を測って、推論した結果、これぐらいの性能になるだろうっていうのをプロットして作ったっていうふうに書かれています。要はこれGPT-4例えばいくつかわかんないっていう現実はわかんないすけど、このパラメータを訓練する前にそれより小さいモデルでスケール則を検証して、これぐらいいくんですねと、これしっかりちゃんと性能上がるんですね。
ちょっと確認したというふうに言われてます。こういった形で自分がモデルを作ろうと思ったときに、モデルを大きくすることに意味があるのかっていうのを見積もることができるようになるわけですよね。これが使い方の一つですね。
ちなみにこれ公開されてないときにちょっと計算してたんですけど、リーク情報出る前にちょっと計算してるんすけどこれ1で何か左端に100ピココンピュートっていうのが書いてあって、多分普通に計算すると、10の10乗より大きくなって、これが1だとしたときにもGPT推定は10Bになるので、100ピコが例えば10の3乗ぐらいのTrasnformerだとしたら少なくとも1Trillionにならなきゃいけないってことで、何かこれで大体推論できるなと思ってました。

その他の使い方としてモデルを、どちらがモデルが良いかっていうのを比較することもできます。これどちらのモデルが良いのかって比較しなければ別に同じパラメータで比較すればいいんじゃないのって思うかもしれないんですけど、もうちょっと厳密に言うと、モデルをスケールさせるとしたらどっちのモデルがいいですかっていうように予測することができるように左側のこの二つの図は最初の前半でも少し話しましたけれど、左側がモデル構造を比較して、右側からパラメータを比較してるものですけど、左側だと例えばTrasnformerの方が、どうやらこれスケールさせてってもずっとよさそうだということがわかります。
LSTMをスケールさせていってもTrasnformerを逆転することは、どうやらなさそうであるということが何となくわかるかなというふうに思います。右側の例だと層を変えてますけど例えば10^5ぐらいしかパラメータがないときに比較すると、これどんなモデルが良いかわからない、どの層がいいかよくわかんないと思うんすけど、このスケール則を測ってると、どうやらパラメータ数を増やしていける前提だったら6層以上に層を増やしていった方が良さそうだということが予想が立つわけです。
このポイントは実際にこれを計算する前にフィッティングしてやればちょっとさっきまでわかるということですね。GPT4の場合と一緒ですけどこれGPT4を実際作る前にこの曲線を引くことができるので、そういう形で小さいパラメータをつくっていろんなパラメータを検証してあって、大きいパラメータでどれぐらい、どういう関係になるかってのを予測することができるっていうのが一つの重要なポイントです。
こうしたやり方はあの研究でもよく使われてまして、これもあのMambaという、研究自体はちょっと紹介しないですけど、あのこのMamba呼ばれる論文から引っ張ってきた図になってます。これ横軸いろいろ書いてるんすけどそれぞれ何か違うモデルですね。
HyenaとかRWKVとか、TrasnformerとかResNetとかH3++って感じで、これがなんかTrasnformerじゃないやつらで試して作られてるものなんですけど、何が言いたいかというと論文でも実際このスケールをさせたときに、どういうふうになりそうかとスケールさせたときに、この提案手法は勝てるのかっていうのが、実際研究されてたりもします。
ちなみにこの論文ちょっと若干余談なんですけどTrasnformerとTrasnformer++っていうのがありまして、これ確かTrasnformer++はLlamaで使われる構造で、Trasnformerが元のGPTの構造なんですけど、これ見ると結構スケール則が違うということもわかると思います。薄いオレンジとオレンジで。なので結構構造が大事だよということも実はこの図でわかったりします。
あとはMambaっていうのがこの紫ってなんか強そうなんですけど、これが何かっていうのは多分Day8でやるんじゃないかなと思ってこれも楽しみにしてもらえばと。

それからちょっと似てる話ですけど効率性を、効率的にやろうと思ったときにどうすればいいのかっていうのを知ることもできます。
左側が横軸がtoken数、縦軸がLoss、色がパラメータに対応しています。これ見ると、例えばこれ若干直感に反することを言ってるところあるんですけど、あるパラメータを固定したときには、基本的にtoken数固定したときには、大きいパラメータのモデルが、サンプル効率がいいロスが下がりやすいということがこの結果からわかったりします。
それから逆に右側が横軸がコンピュートになっていて、色がモデルサイズであることは変わらないんですけどこれを見ると例えば10のマイナス3乗の計算量があるときには、これぐらいのコンピュータを使えばいいということがわかったりします。
これ別に大きければいいというわけではないと。この辺の理屈は小さなモデルだと、学習がロスが下がらなくなるというのでちょっとあのスケール則を書いてるときに、なんか1個1個のプロットを説明したと同じような話ですけど、あの計算量が与えられたときに、どうやら、別にパラメータを増やせばいいわけではないいうことはわかりこの計算量が与えられたときにどれぐらいのバジェット、どれぐらいのパラメータ数とtoken数に割り振ればいいのかっていうのを、あの計算しようとしたのが、先ほど出したこの2変数の関係っていうふうに言ったChinChillaと呼ばれる経験則になってます。


ChinChillaはモデルの名前なんですけどなんかChinChilla則って言われたり、ChinChillaケースって言われたりするので、何かその辺を丸ごと足してChinChillaというふうに、大体呼ばれてると思えばと思います。
左側の図は先ほど見せたのと同じで、それぞれの色が計算量に相当してまして、パラメータ数を変更させた場合です。右側の図が増えてるんですけど、これを各FLOPSで最適なパラメータに直したものっていうのが、この中央でこれを同じようにデータ(token数)に対して、直したものが中央になります。
例えばこれパラメータ見ると何か例えば3E-21を使えるんだったら、あのこの一番下のやつをピックアップしたやつか、この右側の真ん中の方に行ってきていて、同じように1E-12の場合はぴって引っ張ってくるみたいな、やったときにどういう関係があるかっていうので、これを見てみると何となく大体線形っぽい関係にわかります。
右側がtokenの場合の同様の例ですね。これをフィッティングしてると、例えばこれ適当な値ですけど10^24よりちょっと低いぐらいの計算量が使えますよっていうときには、パラメータ数は63Billonにすればいいと。同じところが、これ取られてるんすけど、データ数がどれぐらいすればいいかっていうと、1.4Trillion tokenにすればいいということがわかります。テラtokenですねごめんなさい。
こうして作られたのがChinChillaと呼ばれるモデルになっています。これあのGopherっていうモデルがありまして、GopherがDeepMindがこの前に出してたモデルで、これが280Billonでtoken数が0.3テラtokenというふうになってます。
要はこいつと比べるとこのChinChillaっていうのはモデルサイズがちっちゃいんだけど、あのトレーニングtokenを増やしたと、それはどうやって決まったかというと先ほど言った通りですけど経験則に基づいてどのぐらいのバジェットをパラメータに割り振ってどれぐらいのバジェットをtokenに割り振るかっていうのを、この経験則によって決めた値を使ってやってやるということをしているものです。
結果としてはこれで多くのケースより巨大なモデルに勝てるということが実験上示されています。これ左側のやつが実験上の結果じゃないんで、ちょっと実験結果飲みたい人はこの元の論文を見てもらえれば良いと思いますけど、あの巨大なモデルにかかってるということでこの関係性が良さそうだということが示されています。
ちなみにこれも余談ですけどこのの求め方このChinChilla則ってのは実は何かいくつかの方法で求められてまして、それぞれ大体同じような経験則が出るってことが知られています。この関係式っていうのがよくこれも知られてまして、大体最適なtoken数っていうのが、パラメータ数に20をかけたもの。
これと同じですね。70Billonを訓練するのに1.4とBillonのtokenを使うということになってますのでこの式の20っていうのも謎のマジックナンバーとしてよく出てくるので覚えておくといいと思います。
先ほどPaLM2の図を見せましたけど。PaLM2でも同じような経験則が成立しますよということが言われています。

ここまで見るともうこのtoken数だけであのパラメータ数を、あるパラメータ数に対してtoken数を決めればいいあるいはtoken数が固定データセットサイズが限られてんだったら、それに対してあるパラメータ数で決めてしまえばいいんじゃないかっていうふうに思うかもしれないけど、これはそんなに簡単ではないということも調べています。
何のことですかというか今最適な割り振りっていうふうに言ってたじゃないかというふうに思うかもしれないすけど、一つの観点は、さっき話したのは、訓練のバジェットだけを考えていたという点が、現実的ではあまりないということがあります。これ左がのが訓練時のFLOPSをどうするかっていう話で、ChinChilla Optimalで作ったやつ13Billonのモデルが例えばこんな感じのスケールをしたと。もうちょっとちっちゃいモデルを大量のデータで学習させたモデルは、あのこの黒い黒というか、紺色ですかね、みたいな線だとする。
これロスが同じような値を取るモデルっていうのをピックアップすることができて、この状況で比べるとやっぱり訓練のFLOPSは13Bの方がいいと、ChinChillaOptimalな作り方をする方がいいということが左の図からはわかるんですけど、今度はこの推論するということを考えたのがこの真ん中の図にあります。
これ当然なんですけど、実際にはこのモデル学習した後に、皆さんGPTとかChatGPTとか使ってると思いますけど、みんなが使うときにも計算量がかかるわけです。この推論のときに使うかかるコストっていうのは当然ちっちゃいモデルの方が、小さい、さっき推論のコストが2回分とか2×NDだっていうふうになんかちらっと言いましたけど、それからするとこのコストが推論時にかかり続けるので、学習のコストだけじゃなくて色も考えると、あの7Billonの方が常にちっちゃいわけです。
これが例えばこのぐらいのtokenだったときは、このぐらいのサイズのときに、これ要はゆくゆくは絶対推論コストが低い方が逆転するんですよね。この右側足したやつですけど、そうすると、推論時のコストまで考えてやっていいモデルを作った方がいいんじゃないかっていうのが、あの考えてる人もいたりします。
これがあのBeyond ChinChilla-Optimalというな研究になってる。この論文の結果を一つだけピックアップしましたけど、右上に書いてある数式は6ND_trっていうのが、学習時のtokenに対して6Nさっきのマジックナンバーを掛けたもの。D_infってのがinference時のもので、これに2をかけてるのはバックワードがないので、4回分の計算はないからですね。左の図がなんか綺麗な図が書いてありますけど、Inference tokenを増やしていったときに、ChinChilla係数に対して何倍にしていくかっていうのを、この色が表してます。
これも当たり前なんですけど推論回数が多くなるほど、ライフタイム全体では、あの学習token増やすと要はケースを大きい方にする方が、あの、あるロスを見たときに、あの一番よくないということがわかります。あるtoken数を見た時に一番良いっていうのは変わっていくよっていうことがわかると思います。

こういう考え方もあるねっていう話をしましたけど、これはLlamaとかでも、実際token数を増やすことが多いということにも多分繋がってるっていうふうに言われています。ちょっとさっきの図を持っていましたGopherというのが、DeepMindが元々作ってた巨大なパラメータを少なめのデータセットで学習したものになってまして、ChinChillaっていうのがさっき言った経験則によって学習したものになってます。
これがあの係数が20このtokenをパラメータで割った値が20。20倍のtokenを利用しましょうということでしたけど、例えばLlama2だと学び7Billonでも70Billonと同じtokenを使ってますけど、7Billonのものに関しては1.8Trillionのtoken使ってるので、285倍の係数と全然違う値を使ってる。70Billonは28.5倍。Llama3に関しても、70Billonが214倍、400Billonの方が37倍ということで、このChinChilla-Optimalが実際に巨大なモデル、巨大なtokenの学習で使っているケースもよくあります。


ここまでのまとめが、スケール則ををどう活用するかって話をしてきました。使い方としては投資するかどうか、大きいモデルを開発するかっていうのを割と予測できるというような使い方もありますし、これが実際GPT4の開発にも繋がったというふうに言われています。
モデル選択もできるとパラメータを増やしたときに、どっちが良いモデルかっていうのを実際にパラメータを試すことなく、そのモデルの巨大なモデルを作ることなく検証できるってのがいいとこだと。それからどれぐらい計算資源を割り当てるかっていうのでChinChillaを決めるっていうものがあり、そのChinChillaOptimal以外にも推論コストを考えたときの最適なtokenを議論する研究もなされています。


ここまでがスケール則の活用法でしたけど、少しだけ補足をいくつかしておきたいと思います。この予測可能だというふうに言いました。スケール則ってのは先ほど言ったように、あのべき乗の関係で綺麗なフィッティングができるので、ある意味予測可能なわけです。だけど同時にこのサプライズというか予測不可能な部分っていうのもあるっていうふうにも言われています。これがこれAnthropicっていうOpenAIから独立したというか枝わかれした会社がありますけど、その会社が出してる論文で「Predictability and Surprise in Generative Models」って論文で触れられていますけど、この予測不可能だっていうのがEmergent Abilityと呼ばれるなあの現象になっています。
これ初回で話した通りですけど、例えばパラメータを増やしていったときに、元の計算が突然できるとかそういったものがありますというのが知られています。これ初回でフェアのために何か本当にそうなのかみたいな話を少し言いましたけど、これは実はミラージュ、幻覚なんじゃないかっていうなの研究も出ています。
何を言ってるかっていうといろいろな反証をしてるんですけど、これ要は性能の測り方に寄るでしょうということを言っています。なんか横軸が対数になっていて、うんそもそも何がそのEmergentって言ってるけど、何が創発なんすかみたいなそういった議論もありますけど、少なくとも何かこういう急に何かできるようになるように見えるということが、起こるという期待もこういった言語モデル開発を加速させる要因の一つなのかなというふうに思います。


それからこれもあんまり本題と関係ないですけど面白い現象としてブGrokkingと呼ばれるものを存在しています。これ言語モデル自体ではないんですけど、Grokkingっていうのはめっちゃ過学習させたモデルをずっと学習させ続けると、突然テスト性能があるというような現象になってます。
これはなんかa○bって書いてますけど、こういう2項演算の関係を表してて、aとaだったらaとかこういう何か表があってこの穴埋め問題を解くというようなタスクになってます。この赤線がこの訓練の性能で、緑があのテスト性能なんですけど、横軸これオプティマイズのステップになっていて、なんか10^2とかから10^5とか10のN乗のスケールなんですけど、なんか最初めちゃめちゃ解けないのに、過学習させた状態での学習を続けると性能が上がるというふうに報告されています。
これもある意味予測不能なわけですね計算量を増やし続けると、不思議ことが起こるという現象として知られています。これ何が起こってるのかみたいな研究としてはホットトピックの一つとして知られていて、これなんかrepresentationの学習だっていう、良いrepresentationが学習されると、汎化が起こるんだと。学習し続けるとこういう表現なるんだという研究もありますし、何がこういう良い表現を獲得してるっていうことに繋がってるのかみたいなことも研究されていたりもします。
個人的には面白い時かなと思ったんすけど、今日の話は少し関係ないのでこのぐらいにしておきます。

それから予測不可能な改善という話とも関係するんすけど、実際にはロスを下げたいというよりは何かのダウンストリームの性能を上げたいことが多いと思います。
そういう意味で下流の性能を見た研究もありまして、これは学習以外のデータに対してのテストロスがどうなるかっていうのを見ています。だから仮タスクというよりはデータ分布が変わったときですね。これ見ると大体オフセットが違う実際のロスは違うんですけど、傾きはどうやらスケールしてとかっていうことがわかると、それからこれもコンピュートを指定したときですね横軸にとったときにパラメータはどのぐらいのパラメータだと一番いいかっていうので、これも大体綺麗な関係があるかわかると思います。
一方でこういう軽いタスクが絶対性能が上がるのかっていうとEmergentアビリティの話とだいぶ関係がだいぶ違うこと言ってると思うんですけど、実際にはこのいろんな関係があるっていうことも、検証されています。これ多分GPT4の論文でも議論されているものですし、このGPT3の論文でも言われてるものですけど、綺麗に上がるようなタスクもあれば、急に上がるものもあれば全然上がらないものもあるということで、実際には必ず上がるわけじゃないっていうのは注意してもらえばと思います。
それからこういう何かInverse scaling lawっていうのも知られていて急に悪くなるような、あのタスクも存在するっていうふうに言われていて、去年とかだと何かInverse scaling Prizeっていうことで、スケールさせるほど精度が悪くなるタスクを見つけて賞金が出るみたいな、そういうコンテストも開いていたりします。


具体的な求め方についても話します。
さっきからチラチラ言ってた通りなんすけど基本的にこれどう図るかっていうと、基本的にはいくつかの条件で実験してフィッティングするって言ってんのは、すごい単純に言ってしまうとそうなります。左側GPT4の論文から取ってきた図で説明したもんですけど、グレーのやつを例えば実験してみて、これぐらいのロスになるんだなっていうので、フィッティングするとこういうカーブになります。
ちなみにこれ、なんでこれ直線にならないんだっていうのをすぐ説明しなかったですがこれ縦軸が実は普通のロスと違ってBits-per-wordっていうのになってて、多分2乗スケールのロスになってるからだと思います。
右側も同じですね。この各点について何かいろんな設定で実験してやって、それを結果を見るということをしてますけどよくよく考えるとスケールさせるときにモデルサイズどうすればいいんでしたっけとか、何をどういじるとモデルサイズが大きくなるんでしたっけ、どういうふうに言えばいいんでしたっけとかですね。
あのモデルサイズ変えたときにハイパーパラメータってどうすんでしたっけそういった細かい問題が出てくる。最初の方ですけどモデルサイズどう変化させるかっていうので、前回やった、こういう図があると思いますけどモデルサイズ変えようと思ったら別にパラメータ、層の数を増やしても、いいわけですし、この埋め込みの次元各tokenの次元を増やしてもいいわけですし、各随所に出てくるこのフィードフォワードネットワークっていうのの中間層の次元を上げてもいいですしヘッドを増やしてもそういうのあのパラメータ自体は上がるということで、これどれをどのぐらいやるんですかっていうのが細かく考えると重要になってきます。
この辺は元の論文でも一応議論されてまして、これ三つほど出してるんすけど例えば真ん中のがアスペクト比っていう、モデルのエンベディングのサイズですね。dモデルっていうものを層数で割ったもの、アスペクト比という縦横比みたいなもので幅と深さの比率をアスペクト比っていうふうにこの論文では呼んでいますけど。こういったものを変えて実験してみたっていうのが最初の最初じゃないOpenAIのScaling Lawで話されていました。基本的にはこの辺見るとなんかあんまり性能に影響ないっていうふうにこの論文では言ってますけど、この辺を気にしながらモデルスケールすることが多いです。
気にしながらっていうのの実例を出した方がわかりやすいと思うので、実際にこれ開発者じゃないので、あの結果を見て推論してるだけなんで嘘ついてるかもしれないですけど例えばLlama3の論文を持ってくると8Billon,70Billon,405Billonで層の数、モデルDimension、埋め込みの数次元ですね、フィードフォワードの次元、アテンションの数っていうのを、こういうふうにしたよっていうふうに言われてます。
これさっき言ったアスペクト比、縦横比がこのモデルdimentionをLayerで割ったものなんで、これそれぞれ見ると128,102.4,130ってことでこれ大体100から130ぐらい、なんかおおむね同じような値になっていることがわかると思います。
それからモデルとフィードフォワードの次元数ですね、モデル次元数に対しフィードフォワードの次元数は3.5倍になっているということがわかります。これ約3.5かな。ちょっと自信ないですちょっとちゃんと計算したとかいった計算したら、ちょっと違ってたら教えてほしいんすけど大体3.5倍ぐらいあったとアテンションのヘッドはこのFFNの次元数と同様にスケールしたモデルの次元と同様にスケールしているということがわかる。
こういった感じで幅とかを大体同じような係数で、なるべく伸ばしてくと、ただこれ、指定したパラメータ数にしようと思ったときに、当然どっかは完全には固定できないので、若干変わりますけど大体同じような比率でスケールさせているというようなことがわかると思います。


それからこれCerebrasGPTっていう別の論文ですけどこれの論文を見るとこれちょっとさっきの時Llamaよりだいぶちっちゃいですけど、アスペクト比が大体上から76.8,77.7,85.3,85.3みたいな感じで大体これも同じような値になってて、モデルとFFNの次元がそれぞれ決まっていると、4.0となってて、ヘッドはちょっとこれも論文だとだいぶ規則変化してますけど、大体この辺を揃えているということが経験上わかるかなと。もう一つの話がハイパラをどう変化させるかで、これも同じCerebrasGPTというのを見てみると、例えばこのLR Decayのタイプですね、学習時にDecayをどうするかで、ちっちゃいモデルとlenearにしてんだけど、1.3と2.7はなぜかCosineで6.7はLinearで13BillonはCosineみたいな謎のことをしていることがわかると思う。学習率を見てみると、6.0E-04(6.0掛ける10の-4乗)だったのが1.2*10^-4になってたりこの辺が変化してるということがわかると思います。
これなんでかっていうと、学習したことは方だったら得られたのと、言語モデルに限らずだと思いますけど、あのモデルのサイズとかによって結構この辺の最適なパラメータ数ってのは変わるっていうのは何となくわかるかなと思ってます。
左が何かある論文から持ってきた幅を変化させたときの学習率がどのぐらいだといいのか、最適なのかってのをプロットしたものになってます。色が幅に対応していて、横軸が学習率をlogにしたものですね。これ見ると例えば8192の場合だと、このマイナス16ぐらいにいいとこいますし、128の場合だとマイナス10ぐらいということです。
要は経験則としてモデルサイズを大きくしたとき、大きくしたときには学習率をちっちゃくするってのが大体だということがわかると思います。CerebrasGPTもそうなってます。

バッチサイズを大きくするといいってのも経験則としては言われている。この図はちょっと関係ないですけど、そういう形で実際にはそのパラメータをスケールさせたりする必要があるということに注意してください。ちなみにこれある論文って言ったんすけどμTrasferっていう論文でして、この論文は実はこのパラメータを変える必要があるってことを主張してるわけじゃなくて、何かいい感じの方法を使うといい感じってだいぶ大雑把に言いましたけど、この幅によらず最適なこの学習率の値バッチサイズもそうなんすけど、同じできるよってことを主張してる研究だったりします。
ちょっと面白いので説明したいとこなんすけど興味ある人は、論文見てもらえばと思います。何してるかっていうと簡単に言うと重みの書記官ときとかに、入力の数とかに応じて初期化を変えると思うんですけど、それに似たことを学習率とか、出力のweghtにして掛けてやるとするとこれがweightに対して最適な学習率っていうのがこのケースだけであの最適になるということを示している研究だったりします。
ちょっと興味ある人は読んでもらえばと思い、これμTransferの何か活用方法だけここに行ってますけど、これμPって書いてあるのが、このさっき言った怪しい怪しくはないんですけどただいい、いい感じの初期化をするということをしたものでして、それやると学習率が変わらなくても、いいよってことがCerebrasGPTの場合だと言われています。
ちなみにLlamaの場合はなんかちょっと論文見たんですけどちょっと厳密によくわかんなくてなんか参照したよっていうふうに書いてあるんすけど実際learning rateとは何かちょっといじってるみたいなので、この辺どうやってるかっていうの多分論文とかにあると思うのでちょっとモデルを実際興味ある人は見てもらえばと思います。


トレーニングに関するスケール則の話がここまでで終わりです。基本フィッティングすればいいんですけどモデルサイズをスケールさせるときにはハイパラをどうするかっていうところが、あの結構実験的にやられてまして、モデルサイズについては大体なんか幅アスペクトratioみたいなもの、幅と深さの比率みたいなものを維持しながら受けさせますし、学習率とかはスケジュール、徐々に小さくする、大きくしたときに徐々に着するとか、例としてμTrasnferという技術を使ってたりするということをご理解いただければと思います。


講義に戻ります。ちょっと演習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドってことで「推論時のスケーリング」についての話をして終わろうと思います。モチベーションから話すと、ちょっと頭で考えてみてもらえれば一瞬でわかると思うとんですけど、「バナナの色は何ですかって言われたとき」と、今日の講義聞いた上で、「スケール則の問題は何だと思いますか」って聞かれたとき、多分あの考えることが違うと思うんですね。
「バナナの色なんですか」っていうと黄色ですね。もしかしたら緑かもしれないけど、物によるかなみたいな、おもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも「スケール則の問題なんだと思いますか」って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだから「どうだろう」「この辺が問題かな」みたいな考えとはやっぱ思考としては違うってことは何となく思うかなと思います。
なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。推論時のスケールって言ってるのはこういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、深く考えなきゃいけない問題に対しては、考える時間に計算資源を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。
これの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近o1と呼ばれるモデルがOpenAIから出ました。プレビューとして出てますけどこのo1で注目されています。これあのo1の論文ってかブログにある図で、左側が訓練時の計算資源をスケールさせたときに、AIMEというロジックのベンチマークがあるんですけど、accuracyがどうなったかというと、何となくスケールしてる。右側がtest-time computeっていうふうに書いてると思うんすけど、推論時に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います。
こういった形で、要は考える時間をどうやら推論時に使うと、つまり計算資源を推論時に使うのはいいことがありそうだということがわかります。


そうすると次の話はどうやって計算資源をスケールするのか計算量スケールするのかって話ですけど実は一番簡単な方法はいくつかもうこの講義でも触れていて一つは例えばChain of Thoughtを使うっていうのもその一つです。
Chain of Thoughtを使うと出力するtoken数が増えますよね。その思考の過程を自分で出力するので、これってのはある意味計算資源を増やして使っちゃってると。フェアに言うと、直接答えるのと、tokenをいっぱい出して答えるのは違うわけですよね。ありていに言うとかかるお金が違うわけですからそれからMany Shot ICLっていうのも、あのDay2で多分少し話したのかなと思いますけどIn-Context LearningのときにIn-Contextとして入れる集合を、サイズを増やしていくとどうなるかっていうので、これもスケールしますよということが言われてましたけど、こういった形で簡単にこのPromptingのときに出力するtokenだったり、入力するtokenっていうのをいじってやると、計算量を増やすことができるので、なんでこの辺の研究から見ても明らかなんですけど、基本的にうまく考えるのに計算資源が使えれば性能が上がりますよってのは、既にDay3までにも話してた話だと思います。


それからDay3でもDecodingっていう仕組みを、あの話したと思います。このDecodingにもいろんな方法があってGreedy Decodingだと単純に一番いいやつを選んでいく、一番確率が高いやつ選んでいくので、すごい単純ですけど、こういうトップPを取るとかトップKを取るとかして、最後に一番いいやつを選ぶみたいなことをすると、これも結局計算をたくさんしてることになるわけですね。1個選ぶわけじゃないので、次に評価しなきゃいけないものがどんどん増えていくわけなので、こういった形で増やすっていうのも一つのやり方として存在しています。これ何かDecoding方法の一覧ですけどこれ興味ある人いたら言ってください。


どうしようかな。時間も関係あるんでなんか最近の例を一つだけ紹介するとContrastive Decodingというのは例えばいまして、Extrinsicって書いてあるこの表中の軸に相当する方法の一つ、外部のモデル別のモデルを使う方法なんですけどこの研究によると、単純に自分が自分自身のこの出力の確率を使うよりも、なんかしょぼい言語モデルの確率を割ってやったものを使った方が、厳密にはLogの場合には引いたものですね。この方が良い指標になっているということが知られています。これなんか多分僕、なんかよく話しすぎちゃうものやよく出てくるものじゃないものについてエキスパートモデルが高く値を出していた場合、例えば「1961」ってのは普通はあまり出さないわけですけど、しょぼいモデルも出さないんだったら、これをベースラインとして割ってやる(引いてやる)と良いよっていうようなDecoding方法も出てきています。ここでContrastive Decodingの詳細を理解してほしいということよりは、Decodingのときにその他のモデルを使って性能を上げると、要はこのアマチュア(しょぼい)モデルの計算がさらに増えるわけですけど、こういった形の概念として新しい手法も出てきていたりしますということです。


それからここまでのDecodingの話は基本的に次のtokenをどう選ぶかという話をしてましたけど、Decoding、次のtokenをどう決めるかだけじゃなくて、全体(プロセス全体)としてどういうふうな生成を行うかっていうのを一段広く、上から見ましょうというのでMeta Generationという言い方をするような研究も出てきています。このFrom Decoding Meta Generationというのが、6月かな、に出たサーベイでして、推論時の言語モデルで使うアルゴリズムを広くサーベイしたものになってます。この2章がMeta Generationて概念になってますけどちょっとこれがどういうものかについて説明していきたいと思います。そうね三つぐらい種類があるっていうふうに書かれてます。Parallel Search, Step Level Search, Refinementですけど、ちょっとこれ何か説明より具体を見た方がわかりやすいと思うんで、それぞれ具体を説明していきたいと思います。最後にこれを図を振り返ってこういうもんだなと思ってもらえばいいと思います。
Parallel Searchの方法の一番代表的なのがBest of Nと呼ばれるものです。これ要は、これをtokenだと今までの話と同じなんすけど文を何個か生成してみて一番いいやつを選びましょうっていうのが、このParallel Seacrhの一番簡単なBest of Nっていう方法ですね。一番いいのっていうのがいろいろ考えられて、LLMのスコアを使うっていうのが、これがBeam Searchとかそういう普通のBeam Searchとかに相当するものですけど、valifierと呼ばれるような学習した評価機を使うような研究だったり、GLUEとか機械翻訳のときにGLUEとか、特定の指標を使うみたいに、何かの外部の指標を使って、とりあえずN個生成して、一番いいやつを、後で選びましょうとそういうアプローチになってます。ちょっとMBR Decodingの話しようかと思うんすけど、時間がだいぶ迫ってるかもしれないので簡単に言うと、これがMBR Decodingというのが最近機械翻訳で注目されてる一つの方法らしいんですけど、この例の中で言うとGLUEとかの指標を使っていいものを選ぶというような研究の例になってます。そうですね今言った通りですけどこの何か期待値の中にUて書いてあるけど、このUっていうのが、このさっきこのスライドでのスコアに相当してましてGLUEとかMeteorとか、いろんなものを使っています。これは実際にこれ人間のサンプルを用いたこういう関数が必要なんですけど、これをこのMBRだとやめていてHumanの代わりにモデルから生成したものをサンプリングして期待値を取るみたいなことをやるのがMBR Decodingというやれるやり方になってるんすけど、ちょっと詳細が知りたい人は、だいぶわかりやすい資料が上がってましたのでこのURLから飛んでもらえばと思います。ここではこの、要はスコアを選んでいいものを選ぶというようなやり方でいろいろ発展してきてるんだということを理解していただければいいと思います。


それからBest of Nとはちょっと違う方法として、N個を生成した後に、それらを集約するという意味では、Day2でやったSelf-Consistencyをこの枠組みの一つとして説明されます。Self-Consistencyは下のようなもんですけど推論のCoTを応用したものになってて、言語モデルにCoTでのいろんなReasoning Pathを出させて、"Marginalize out reasoning paths"って書いてあるけどそのreasoning pathを出した後にこの一番よく出てきた答えっていうのを最終的に答えとするというようなやり方になってると思います。
これもN個の中から1個選んでるわけじゃないんですけどN個出力して、それらを集約する形でこのN個はそれぞれ独立に動く(パラレルに動く)ので、さらに動かして最後集約するという意味でこのParallel Searchの枠組みの一つの代表的な手法になってます。このアグリゲーションどういうふうにこの結果を集約するかだったり、どういうふうにスコアをつけるかっていうので、いろんな手法が知られてましてこれもさっきのサーベイ論文に入っているので興味を引いていけばてもらえればと思います。


ここまでパラレルサーチの中でMajority VotingとBest of Nっていうのが出てきましたけど、これあるタスクこれ確か数学のタスクだと思うんすけどタスクが載ってないんで興味ある人はこの論文見てもらえばと思うんすけど、例えば比較すると、普通にMajority Votingするのがこの黒線で、あの青線がこのBest of N、ORMって書いてあるけど、これOutcome-supervised Reward Modelってもので、要は全体に対して、これが正しいか正しくないかを推定するリワードモデル、結果をすいてするReward Modelってのを用意してあって、Reward Modelにとって一番良かったやつを最後選ぶっていうそういうやり方になってます。Majority VotingはN個出して最大のものを選ぶっていうやり方です。これやると比べると、リワードモデル使った方がいいよと言われています。一応補足なんですがMajority Votingは別にそのリワードモデルとか作る必要ないので簡単な方法であるっていう利点はあって、best of Nの方のORMっていうのは、リワードモデルを別で学習しなきゃいけないってのが欠点としてあることは一応補足しておきます。


それからこのオレンジの線をスルーして話したんですけど実際にはこの論文はこのオレンジのやり方を提案してる論文になってます。それがこのPRMっていうものでして、PRMってのはProcess-Supervised Reward Modelっていうふうに呼ばれています。あるいは論文によってはProcess Reward Modelって普通に読んでるものもあります。これは全体に対して合ってる間違ってるっていうのを予測するんじゃなくて、このプロセスごとに合ってる間違ってるみたいのを予測するモデルを作るというものです。これあの数学の問題みたいなプロセスがわかりやすいですけど、最初に"Let's call the numerator x"みたいな感じでそれぞれのパスが正しいか正しくないかみたいな、これはユーザーが提出してる例ですけど、ユーザーの提出した例を使ってプロセスの正しさを予測してるということをしています。これをやるとさらに性能が上がるっていうことが左の図からわかりまして、これちゃんと説明しなかったですけど横軸がN個なんで生成するものの数になってますけど、これ増やしていくとさらに性能が上がるということがわかります。このPRMみたいなプロセスに対して評価を付けるっていうな説明をしましたけど、こういったやり方をするのがStep Level Searchっていうふうにさっきのサーベイ論文にまとめられています。ごこの一つ前の研究ではこのProcess Reward ModelのRewardを使って、あのBest of Nを選ぶってやり方をしましたけど、Best of N以外にも、このプロセスごとに塊を作って生成していってビームサーチみたいなことをすることもできます。要は最初はこの2個が良さそうだからこの2個を生き残らせて、こいつからまた発生させて2個作ってまた次のステップ3の選んでみたいな。これを繰り返すっていう方法です。これ丸が一つのプロセス、tokenじゃなくてプロセスに対応してましてというのが普通のBeam Searchの違いですけどそういった研究もあります。ちなみにそういった研究もありますって言いましたけどこのTree of SearchっていうのをDay2で多分やったと思うんすけどこれがほぼ似たようなことをしています。みたいなことっていうのはこのリワードモデルっていうの使う代わりに言語モデルで、この状態がいいものか、こっからこれGame of 24の例ですけど、足して24になる数字の作り方をしてくださいっていうときに、あのもう絶対に達成できない場合は、あの言語モデルがFailとするというようなことをしているものですけど、そういった形でやってるっていうのも、言語モデルをあのリワードの評価として使っているステップレベルサーチの例だと思っていただければと思います。これもさっきと同様ですけど探索方法とか検証するステップの違いですね。どこで検証するかとかによっていろんな方法が知られています。これもちょっと時間の関係で全体を割愛しますけど、これもサーベイ論文にあるので読んでもらいたいと思います。


それから全然別のやり方でRefinementと呼ばれるようなやり方もあります。これ概念図だけピックアップしましたけど、右側の黒いのが対象の値でこれを言語モデルが生成したものだとしたときに、これ自身を入れてやって、もっかい生成させるというようなことだったり、あるいはこの生成したものに対してフィードバックを、左側の白いボックスに相当しますけど与えてやって再度生成するみたいな、そういったやり方をする研究もあります。これがリファインメントというようなやり方になってます。このリファイベントの代表的な研究がSelf-Refineと言われるような研究がありまして左が概念図でさっきとほぼ同じですけど、なんか最初タスクを言語モデルに与えましたと、こいつが出力した結果をSelf-Refineという研究だと自分自身で評価して、あとフィードバックを返すと。そのフィードバックを行った結果を使ってもっかいRefineすると、このRefineもこのSelf-Refineでは自分自身でやるんすけど、そういった枠組みの研究があります。右側例えばの生成例で上側のABCが一つの系列になってますけど、ダイアログが与えられたと会話が与えられたときに、このフィードバックって書いてあるのが、この左側の①のプロセスに相当するもの。これも言語モデルが出してるものでこのフィードバックを踏まえてリファインしたのがこの右側のものになります。ちょっと中身は読まないですけどこういった形で最初に生成させ、それ何を変えるべきかっていうのも言語モデル生成して、最後にその何を変えるべきかというのを踏まえてもう一度生成すると。これを何回も何回も繰り返すっていうのはやり方になってます。こんなんで性能上がるのかって思うと思うんですけど、これが何か上がるらしくてですね、最大50%ぐらい上がるような例もあるっていうふうに言われています。結果は結果なので、一旦これぐらいにします。


これでほぼちょうどですけど、最後に少しあの、前半では全体の訓練時のスケーリングをする話を基本的にしましたけど、最近ではこの推論時の計算量っていうのも注目するような研究が増えてきています。代表的なGPT-o1とかですごく注目されてるかなと思いますし、今までやった方法、学んだ方法も結構出てきたと思いますけど、Promptingを工夫するとか、Decodingを工夫するとかいうので、それにも発展的な方法がいろいろ出てきていますし、Meta Generationっていうような枠組みで、DecodingだけじゃなくてそのDecodeした結果を最後どう使うかみたいな含めて、Meta Generationというふうに呼んでますけど、Paralell SearchとかStep Level SearchとかRefinementと言われるような枠組みの研究も出てきていますというような話をしました。


最後に補足して僕のパート終わろうと思いますけど、同じ計算資源のときにパラメータ増やすのよりも推論資源を増やすのが有効なのかっていうのが問いとしてあると思いますけど、o1の場合だと、訓練時のスケールは同じままって推論時のスケールを増やしたら、より賢くなりましたって話でしたけど、どっちにするのがいいのかっていう意味で言うと、GoogleDeepMindが8月に論文としてまして、Scaling LLM Test-Time Comupte Optimally can be more Effective than Scaling More Paremetersっていうことで、良いらしいというふうに言われてます。厳密に言うとこれなんかタスクによって違うということなので、良いとまで言っていいのかちょっと若干誇大広告な気が個人的にはしてますけど、そういったことを検証するような研究も出てきていますので興味ある人は見てもらえばと思います。