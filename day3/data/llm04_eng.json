[
    {
        "content": "それからこれCerebrasGPTっていう別の論文ですけどこれの論文を見るとこれちょっとさっきの時Llamaよりだいぶちっちゃいですけど、アスペクト比が大体上から76.8,77.7,85.3,85.3みたいな感じで大体これも同じような値になってて、モデルとFFNの次元がそれぞれ決まっていると、4.0となってて、ヘッドはちょっとこれも論文だとだいぶ規則変化してますけど、大体この辺を揃えているということが経験上わかるかなと。もう一つの話がハイパラをどう変化させるかで、これも同じCerebrasGPTというのを見てみると、例えばこのLR Decayのタイプですね、学習時にDecayをどうするかで、ちっちゃいモデルとlenearにしてんだけど、1.3と2.7はなぜかCosineで6.7はLinearで13BillonはCosineみたいな謎のことをしていることがわかると思う。学習率を見てみると、6.0E-04(6.0掛ける10の-4乗)だったのが1.2*10^-4になってたりこの辺が変化してるということがわかると思います。"
    },
    {
        "content": "これなんでかっていうと、学習したことは方だったら得られたのと、言語モデルに限らずだと思いますけど、あのモデルのサイズとかによって結構この辺の最適なパラメータ数ってのは変わるっていうのは何となくわかるかなと思ってます。"
    },
    {
        "content": "左が何かある論文から持ってきた幅を変化させたときの学習率がどのぐらいだといいのか、最適なのかってのをプロットしたものになってます。色が幅に対応していて、横軸が学習率をlogにしたものですね。これ見ると例えば8192の場合だと、このマイナス16ぐらいにいいとこいますし、128の場合だとマイナス10ぐらいということです。"
    },
    {
        "content": "要は経験則としてモデルサイズを大きくしたとき、大きくしたときには学習率をちっちゃくするってのが大体だということがわかると思います。CerebrasGPTもそうなってます。バッチサイズを大きくするといいってのも経験則としては言われている。この図はちょっと関係ないですけど、そういう形で実際にはそのパラメータをスケールさせたりする必要があるということに注意してください。ちなみにこれある論文って言ったんすけどμTrasferっていう論文でして、この論文は実はこのパラメータを変える必要があるってことを主張してるわけじゃなくて、何かいい感じの方法を使うといい感じってだいぶ大雑把に言いましたけど、この幅によらず最適なこの学習率の値バッチサイズもそうなんすけど、同じできるよってことを主張してる研究だったりします。"
    },
    {
        "content": "ちょっと面白いので説明したいとこなんすけど興味ある人は、論文見てもらえばと思います。何してるかっていうと簡単に言うと重みの書記官ときとかに、入力の数とかに応じて初期化を変えると思うんですけど、それに似たことを学習率とか、出力のweghtにして掛けてやるとするとこれがweightに対して最適な学習率っていうのがこのケースだけであの最適になるということを示している研究だったりします。"
    },
    {
        "content": "ちょっと興味ある人は読んでもらえばと思い、これμTransferの何か活用方法だけここに行ってますけど、これμPって書いてあるのが、このさっき言った怪しい怪しくはないんですけどただいい、いい感じの初期化をするということをしたものでして、それやると学習率が変わらなくても、いいよってことがCerebrasGPTの場合だと言われています。"
    },
    {
        "content": "ちなみにLlamaの場合はなんかちょっと論文見たんですけどちょっと厳密によくわかんなくてなんか参照したよっていうふうに書いてあるんすけど実際learning rateとは何かちょっといじってるみたいなので、この辺どうやってるかっていうの多分論文とかにあると思うのでちょっとモデルを実際興味ある人は見てもらえばと思います。"
    },
    {
        "content": "スケール則の話がここまでで終わりなのがトレーニングですけどそこの話終わりますけど、基本フィッティングすればいいんですけどモデルサイズをスケールさせるときにはハイパラをどうするかっていうところが、あの結構実験的にやられてまして、モデルサイズについては大体なんか幅アスペクトratioみたいなもの、幅と深さの比率みたいなものを維持しながら受けさせますし、学習率とかはスケジュール、徐々に小さくする、大きくしたときに徐々に着するとか、例としてμTrasnferという技術を使ってたりするということをご理解いただければと思います。"
    },
    {
        "content": "講義に戻ります。ちょっと演習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドってことで「推論時のスケーリング」についての話をして終わろうと思います。モチベーションから話すと、ちょっと頭で考えてみてもらえれば一瞬でわかると思うとんですけど、「バナナの色は何ですかって言われたとき」と、今日の講義聞いた上で、「スケール則の問題は何だと思いますか」って聞かれたとき、多分あの考えることが違うと思うんですね。「バナナの色なんですか」っていうと黄色ですね。もしかしたら緑かもしれないけど、物によるかなみたいな、おもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも「スケール則の問題なんだと思いますか」って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだから「どうだろう」「この辺が問題かな」みたいな考えとはやっぱ思考としては違うってことは何となく思うかなと思います。なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。推論時のスケールって言ってるのはこういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、深く考えなきゃいけない問題に対しては、考える時間に計算資源を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。これの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近o1と呼ばれるモデルがOpenAIから出ました。プレビューとして出てますけどこのo1で注目されています。これあのo1の論文ってかブログにある図で、左側が訓練時の計算資源をスケールさせたときに、AIMEというロジックのベンチマークがあるんですけど、accuracyがどうなったかというと、何となくスケールしてる。右側がtest-time computeっていうふうに書いてると思うんすけど、推論時に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います。こういった形で、要は考える時間をどうやら推論時に使うと、つまり計算資源を推論時に使うのはいいことがありそうだということがわかります。"
    },
    {
        "content": "そうすると次の話はどうやって計算資源をスケールするのか計算量スケールするのかって話ですけど実は一番簡単な方法はいくつかもうこの講義でも触れていて一つは例えばChain of Thoughtを使うっていうのもその一つです。Chain of Thoughtを使うと出力するtoken数が増えますよね。その思考の過程を自分で出力するので、これってのはある意味計算資源を増やして使っちゃってると。フェアに言うと、直接答えるのと、tokenをいっぱい出して答えるのは違うわけですよね。ありていに言うとかかるお金が違うわけですからそれからMany Shot ICLっていうのも、あのDay2で多分少し話したのかなと思いますけどIn-Context LearningのときにIn-Contextとして入れる集合を、サイズを増やしていくとどうなるかっていうので、これもスケールしますよということが言われてましたけど、こういった形で簡単にこのPromptingのときに出力するtokenだったり、入力するtokenっていうのをいじってやると、計算量を増やすことができるので、なんでこの辺の研究から見ても明らかなんですけど、基本的にうまく考えるのに計算資源が使えれば性能が上がりますよってのは、既にDay3までにも話してた話だと思います。"
    },
    {
        "content": "それからDay3でもDecodingっていう仕組みを、あの話したと思います。このDecodingにもいろんな方法があってGreedy Decodingだと単純に一番いいやつを選んでいく、一番確率が高いやつ選んでいくので、すごい単純ですけど、こういうトップPを取るとかトップKを取るとかして、最後に一番いいやつを選ぶみたいなことをすると、これも結局計算をたくさんしてることになるわけですね。1個選ぶわけじゃないので、次に評価しなきゃいけないものがどんどん増えていくわけなので、こういった形で増やすっていうのも一つのやり方として存在しています。これ何かDecoding方法の一覧ですけどこれ興味ある人いたら言ってください。"
    },
    {
        "content": "どうしようかな。時間も関係あるんでなんか最近の例を一つだけ紹介するとContrastive Decodingというのは例えばいまして、Extrinsicって書いてあるこの表中の軸に相当する方法の一つ、外部のモデル別のモデルを使う方法なんですけどこの研究によると、単純に自分が自分自身のこの出力の確率を使うよりも、なんかしょぼい言語モデルの確率を割ってやったものを使った方が、厳密にはLogの場合には引いたものですね。この方が良い指標になっているということが知られています。これなんか多分僕、なんかよく話しすぎちゃうものやよく出てくるものじゃないものについてエキスパートモデルが高く値を出していた場合、例えば「1961」ってのは普通はあまり出さないわけですけど、しょぼいモデルも出さないんだったら、これをベースラインとして割ってやる(引いてやる)と良いよっていうようなDecoding方法も出てきています。ここでContrastive Decodingの詳細を理解してほしいということよりは、Decodingのときにその他のモデルを使って性能を上げると、要はこのアマチュア(しょぼい)モデルの計算がさらに増えるわけですけど、こういった形の概念として新しい手法も出てきていたりしますということです。"
    },
    {
        "content": "それからここまでのDecodingの話は基本的に次のtokenをどう選ぶかという話をしてましたけど、Decoding、次のtokenをどう決めるかだけじゃなくて、全体(プロセス全体)としてどういうふうな生成を行うかっていうのを一段広く、上から見ましょうというのでMeta Generationという言い方をするような研究も出てきています。このFrom Decoding Meta Generationというのが、6月かな、に出たサーベイでして、推論時の言語モデルで使うアルゴリズムを広くサーベイしたものになってます。この2章がMeta Generationて概念になってますけどちょっとこれがどういうものかについて説明していきたいと思います。そうね三つぐらい種類があるっていうふうに書かれてます。Parallel Search, Step Level Search, Refinementですけど、ちょっとこれ何か説明より具体を見た方がわかりやすいと思うんで、それぞれ具体を説明していきたいと思います。最後にこれを図を振り返ってこういうもんだなと思ってもらえばいいと思います。"
    },
    {
        "content": "Parallel Searchの方法の一番代表的なのがBest of Nと呼ばれるものです。これ要は、これをtokenだと今までの話と同じなんすけど文を何個か生成してみて一番いいやつを選びましょうっていうのが、このParallel Seacrhの一番簡単なBest of Nっていう方法ですね。一番いいのっていうのがいろいろ考えられて、LLMのスコアを使うっていうのが、これがBeam Searchとかそういう普通のBeam Searchとかに相当するものですけど、valifierと呼ばれるような学習した評価機を使うような研究だったり、GLUEとか機械翻訳のときにGLUEとか、特定の指標を使うみたいに、何かの外部の指標を使って、とりあえずN個生成して、一番いいやつを、後で選びましょうとそういうアプローチになってます。ちょっとMBR Decodingの話しようかと思うんすけど、時間がだいぶ迫ってるかもしれないので簡単に言うと、これがMBR Decodingというのが最近機械翻訳で注目されてる一つの方法らしいんですけど、この例の中で言うとGLUEとかの指標を使っていいものを選ぶというような研究の例になってます。そうですね今言った通りですけどこの何か期待値の中にUて書いてあるけど、このUっていうのが、このさっきこのスライドでのスコアに相当してましてGLUEとかMeteorとか、いろんなものを使っています。これは実際にこれ人間のサンプルを用いたこういう関数が必要なんですけど、これをこのMBRだとやめていてHumanの代わりにモデルから生成したものをサンプリングして期待値を取るみたいなことをやるのがMBR Decodingというやれるやり方になってるんすけど、ちょっと詳細が知りたい人は、だいぶわかりやすい資料が上がってましたのでこのURLから飛んでもらえばと思います。ここではこの、要はスコアを選んでいいものを選ぶというようなやり方でいろいろ発展してきてるんだということを理解していただければいいと思います。"
    },
    {
        "content": "それからBest of Nとはちょっと違う方法として、N個を生成した後に、それらを集約するという意味では、Day2でやったSelf-Consistencyをこの枠組みの一つとして説明されます。Self-Consistencyは下のようなもんですけど推論のCoTを応用したものになってて、言語モデルにCoTでのいろんなReasoning Pathを出させて、'Marginalize out reasoning paths'って書いてあるけどそのreasoning pathを出した後にこの一番よく出てきた答えっていうのを最終的に答えとするというようなやり方になってると思います。これもN個の中から1個選んでるわけじゃないんですけどN個出力して、それらを集約する形でこのN個はそれぞれ独立に動く(パラレルに動く)ので、さらに動かして最後集約するという意味でこのParallel Searchの枠組みの一つの代表的な手法になってます。このアグリゲーションどういうふうにこの結果を集約するかだったり、どういうふうにスコアをつけるかっていうので、いろんな手法が知られてましてこれもさっきのサーベイ論文に入っているので興味を引いていけばてもらえればと思います。"
    },
    {
        "content": "ここまでパラレルサーチの中でMajority VotingとBest of Nっていうのが出てきましたけど、これあるタスクこれ確か数学のタスクだと思うんすけどタスクが載ってないんで興味ある人はこの論文見てもらえばと思うんすけど、例えば比較すると、普通にMajority Votingするのがこの黒線で、あの青線がこのBest of N、ORMって書いてあるけど、これOutcome-supervised Reward Modelってもので、要は全体に対して、これが正しいか正しくないかを推定するリワードモデル、結果をすいてするReward Modelってのを用意してあって、Reward Modelにとって一番良かったやつを最後選ぶっていうそういうやり方になってます。Majority VotingはN個出して最大のものを選ぶっていうやり方です。これやると比べると、リワードモデル使った方がいいよと言われています。一応補足なんですがMajority Votingは別にそのリワードモデルとか作る必要ないので簡単な方法であるっていう利点はあって、best of Nの方のORMっていうのは、リワードモデルを別で学習しなきゃいけないってのが欠点としてあることは一応補足しておきます。"
    },
    {
        "content": "それからこのオレンジの線をスルーして話したんですけど実際にはこの論文はこのオレンジのやり方を提案してる論文になってます。それがこのPRMっていうものでして、PRMってのはProcess-Supervised Reward Modelっていうふうに呼ばれています。あるいは論文によってはProcess Reward Modelって普通に読んでるものもあります。これは全体に対して合ってる間違ってるっていうのを予測するんじゃなくて、このプロセスごとに合ってる間違ってるみたいのを予測するモデルを作るというものです。これあの数学の問題みたいなプロセスがわかりやすいですけど、最初にLet's call the numerator xみたいな感じでそれぞれのパスが正しいか正しくないかみたいな、これはユーザーが提出してる例ですけど、ユーザーの提出した例を使ってプロセスの正しさを予測してるということをしています。これをやるとさらに性能が上がるっていうことが左の図からわかりまして、これちゃんと説明しなかったですけど横軸がN個なんで生成するものの数になってますけど、これ増やしていくとさらに性能が上がるということがわかります。このPRMみたいなプロセスに対して評価を付けるっていうな説明をしましたけど、こういったやり方をするのがStep Level Searchっていうふうにさっきのサーベイ論文にまとめられています。ごこの一つ前の研究ではこのProcess Reward ModelのRewardを使って、あのBest of Nを選ぶってやり方をしましたけど、Best of N以外にも、このプロセスごとに塊を作って生成していってビームサーチみたいなことをすることもできます。要は最初はこの2個が良さそうだからこの2個を生き残らせて、こいつからまた発生させて2個作ってまた次のステップ3の選んでみたいな。これを繰り返すっていう方法です。これ丸が一つのプロセス、tokenじゃなくてプロセスに対応してましてというのが普通のBeam Searchの違いですけどそういった研究もあります。ちなみにそういった研究もありますって言いましたけどこのTree of SearchっていうのをDay2で多分やったと思うんすけどこれがほぼ似たようなことをしています。みたいなことっていうのはこのリワードモデルっていうの使う代わりに言語モデルで、この状態がいいものか、こっからこれGame of 24の例ですけど、足して24になる数字の作り方をしてくださいっていうときに、あのもう絶対に達成できない場合は、あの言語モデルがFailとするというようなことをしているものですけど、そういった形でやってるっていうのも、言語モデルをあのリワードの評価として使っているステップレベルサーチの例だと思っていただければと思います。これもさっきと同様ですけど探索方法とか検証するステップの違いですね。どこで検証するかとかによっていろんな方法が知られています。これもちょっと時間の関係で全体を割愛しますけど、これもサーベイ論文にあるので読んでもらいたいと思います。"
    },
    {
        "content": "それから全然別のやり方でRefinementと呼ばれるようなやり方もあります。これ概念図だけピックアップしましたけど、右側の黒いのが対象の値でこれを言語モデルが生成したものだとしたときに、これ自身を入れてやって、もっかい生成させるというようなことだったり、あるいはこの生成したものに対してフィードバックを、左側の白いボックスに相当しますけど与えてやって再度生成するみたいな、そういったやり方をする研究もあります。これがリファインメントというようなやり方になってます。このリファイベントの代表的な研究がSelf-Refineと言われるような研究がありまして左が概念図でさっきとほぼ同じですけど、なんか最初タスクを言語モデルに与えましたと、こいつが出力した結果をSelf-Refineという研究だと自分自身で評価して、あとフィードバックを返すと。そのフィードバックを行った結果を使ってもっかいRefineすると、このRefineもこのSelf-Refineでは自分自身でやるんすけど、そういった枠組みの研究があります。右側例えばの生成例で上側のABCが一つの系列になってますけど、ダイアログが与えられたと会話が与えられたときに、このフィードバックって書いてあるのが、この左側の①のプロセスに相当するもの。これも言語モデルが出してるものでこのフィードバックを踏まえてリファインしたのがこの右側のものになります。ちょっと中身は読まないですけどこういった形で最初に生成させ、それ何を変えるべきかっていうのも言語モデル生成して、最後にその何を変えるべきかというのを踏まえてもう一度生成すると。これを何回も何回も繰り返すっていうのはやり方になってます。こんなんで性能上がるのかって思うと思うんですけど、これが何か上がるらしくてですね、最大50%ぐらい上がるような例もあるっていうふうに言われています。結果は結果なので、一旦これぐらいにします。"
    },
    {
        "content": "これでほぼちょうどですけど、最後に少しあの、前半では全体の訓練時のスケーリングをする話を基本的にしましたけど、最近ではこの推論時の計算量っていうのも注目するような研究が増えてきています。代表的なGPT-o1とかですごく注目されてるかなと思いますし、今までやった方法、学んだ方法も結構出てきたと思いますけど、Promptingを工夫するとか、Decodingを工夫するとかいうので、それにも発展的な方法がいろいろ出てきていますし、Meta Generationっていうような枠組みで、DecodingだけじゃなくてそのDecodeした結果を最後どう使うかみたいな含めて、Meta Generationというふうに呼んでますけど、Paralell SearchとかStep Level SearchとかRefinementと言われるような枠組みの研究も出てきていますというような話をしました。"
    },
    {
        "content": "最後に補足して僕のパート終わろうと思いますけど、同じ計算資源のときにパラメータ増やすのよりも推論資源を増やすのが有効なのかっていうのが問いとしてあると思いますけど、o1の場合だと、訓練時のスケールは同じままで推論時のスケールを増やしたら、より賢くなりましたって話でしたけど、どっちにするのがいいのかっていう意味で言うと、GoogleDeepMindが8月に論文としてまして、Scaling LLM Test-Time Comupte Optimally can be more Effective than Scaling More Paremetersっていうことで、良いらしいというふうに言われてます。厳密に言うとこれなんかタスクによって違うということなので、良いとまで言っていいのかちょっと若干誇大広告な気が個人的にはしてますけど、そういったことを検証するような研究も出てきていますので興味ある人は見てもらえばと思います。"
    }
]