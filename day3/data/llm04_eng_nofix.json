[
    {
        "content": "それからこれセレブらすGPTっていう別の論文ですけどこれの論文を見るとこれちょっとさっきの時Llamaよりだいぶちっちゃいですけど、アスペクト比が大体上から76.877.785.385.3みたいな感じで大体これも同じような値になってて、モデルとFN次元がそれぞれ決まっていると、4.0となってて、ヘッドはちょっとこれもLaMDAとだいぶ規則変化してますけど、大体この辺を揃えているということが経験上わかるかなともう一つの話がハイパーをどう変化させるかで、これも同じ3プラスGPTというのを見てみると、例えばこのLRDKのタイプ理系ですね各種時に理系をどうするかで、ちっちゃいモデルとリニアにしてんだけど、1.3と2点弾はなぜかコサインで6.7リニアで小さいBillonはコサインみたいな謎のことをしていることがわかると思う学習率を見てみると、6.0E-野木6.090-4乗だったのが一点に影っていうのマイナス4乗になってたりこの辺が変化してるということがわかると思います。これなんでかっていうと、学習したことは方だったら得られたのと、言語モデルに限らずだと思いますけど、あのモデルのサイズとかによって結構この辺の最適なパラメータ数ってのは変わるっていうのは何となくわかるかなと思ってます。左が何かある論文から持ってきた幅を変化させたときの学習率がどのぐらいだといいのか、最適なのかってのをプロットしたものになってます。色が幅に対応していて、横軸が学習率を6にしたものですね。これ見ると例えば8192の場合だと、このマイナス16ぐらいにいいとこいますし、128の場合だとマイナス10ぐらいということです。要は経験則としてモデルサイズを大きくしたとき、大きくごめんなさい。大きくなってるのが大きい大きくしたときには学生とちっちゃくするってのが大体だということがわかると思います。SalesGPTもそうなってますバッチサイズを大きくするといいってのも経験則としては言われているこの図はちょっと関係ないですけどそういう形で実際にはそのパラメータをスケールさせたりする必要があるということに注意してください。ちなみにこれある論文って言ったんすけどμTrasnformerGopherっていう論文でして、この論文は実はこのパラメータを変える必要があるってことを主張してるわけじゃなくて、何かいい感じの方法を使うといい感じってだいぶ大雑把に言いましたけど、この幅によらず最適なこの学習率の値バッチサイドもそうなんすけど、同じできるよってことを主張してる研究だったりします。ちょっと面白いので説明したいとこなんすけど興味ある人は、論文見てもらえばと思います何してるかっていうと簡単に言うと、れるじゃないや、あの重みの書記官ときとかに、入力の数とかに応じて初期化を変えると思うんですけど、それにいたことを学習率とか、努力のウエートにして掛けてやるとするとこれがウエイトに対して最適な学習率っていうのがこのケースだけであの最適になるということを示している研究だったりします。ちょっと興味ある人は読んでもらえばと思い、これm3の何か活用方法だけここに行ってますけど、これUPって書いてあるのが、このさっき言った怪しい怪しくはないんですけどただいい、いい感じの初期化をするということをしたものでしてそれやると学習室が変わらなくても、いいよってことがSalesGPTの場合だと言われています。ちなみにLlamaの場合はなんかちょっと論文見たんですけどちょっと厳密によくわかんなくてなんか参照したよっていうふうに書いてあるんすけど実際ラーニング例とは何かちょっといじってるみたいなので、この辺どうやってるかっていうの多分論文とか図形なモデルによると思うのでちょっとモデルを実際興味ある人は見てもらえばと思います。"
    },
    {
        "content": "スケール則の話がここまでで終わりなのがトレーニングですけどそこの話終わりますけど、基本フィッティングすればいいんですけどモデルサイズを気にする方モデル再度スケールさせるときにはハイパーどうするかっていうところが、あの結構実験的にやられてまして、ボディサイズについては大体なんか幅アスペクト例集みたいなもの、ぱぱっと深さの比率みたいなものを維持しながら受けさせますし、学習率とかはスケジュール、明日への徐々に小さくする、大きくしたときに徐々に着するとか、ある意味Trasnformerという技術を使ってたりするということをご理解いただければと思います。"    },
    {
        "content": "講義に戻ります。ちょっと練習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドって話で生のGENIACLMの話をして終わろうと思いですねちょっとモチベーションから話すと、ちょっと頭で考えてみてほしいとか見れば一瞬で思うとんですけどバナナの色は何ですかって言われたときと、今日の講義聞いた上で、ゲームソフトの問題は何だと思いますかって聞かれたとき、多分あの考えることが違うと思うんですね。羽の色なんですかっていうと一瞬黄色ですねもしかしたら緑かもしれないけどぐらいですかね物によるかなみたいなおもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくともスケール足の問題なんだと思いますかって聞かれたときに、今日の話からするとスケール則っていうのはこういうものだからどうだろうこの辺が問題かなみたいな考えとやっぱ思考としては違うってことは何となく思うかなと思います。なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。スケールって言ってる7Gのスケールって言ってるのはこういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、あの考えなきゃいけない問題に対しては、考える時間を、に計算式を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。二つで、ちょっと順番が前後しますけどこれの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近大湾と呼ばれる論文が論文じゃないか、モデルがオペルから出ましたプレビューとして出てますけどこの法案で注目されていますこれあの論文にROMってかブログにあるとイエスって右側が訓練時の計算資源をスケールさせたときに、初めて何かロジックのベンチマークがあるんですけどこれをがどうなったかで何となくスケールしてると右側がテストTimeコンピュートっていうふうに書いてると思うんすけど、水温時に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだんコース計算式を増やしていったときに、性能がどう変わるかっていうのでこれもスケールしていってるということがわかると思います。こういった形で、要は考える時間をどうやら推論時に使うと計算資源を推論使うのはいいことがありそうだということがわかります。"    },
    {
        "content": "そうすると次の話はどうやって計算資源をスケールするのか計算量スケールするのかって話ですけど実は一番簡単な方法はいくつかもうこの講義でも触れていて一つは例えばチェーン相当使うっていうのもその一つです。チャームソフトを使うと出力するプロtoken数が増えますよね。その思考の過程を自分で出力するので、これってのはある意味計算資源を増やして使っちゃってるとフェアに言うと、直接答えるのと、tokenをいっぱい出して使う。答えるのは違うわけですよね。ありていに言うとかかるお金が違うわけですからそれから目にちょっとICLっていうのも、あのDay2で多分少し話したのかなと思いますけどPretrainingときに学習テキストに入れるQを、サイズを増やしていくとどうなるかっていうのでこれもスケールしますよということが言われてましたけどこういった形で簡単にこのPromptingのときに出力するtokenだったり、入力するtokenっていうのをいじってやると、計算量を増やすことができるので、なんでこの辺の研究から見ても明らかなんですけど、本当に間うまく考えるのに計算式が使えれば性能が上がりますよってのは、既にあの子のあのSTまでにも話してた話だと思います。"
    },
    {
        "content": "それからDay3でもDecodingっていう仕組みを、あの話したと思います。このDecodingにもいろんな方法があってグリーンDecodingだと単純に一番いいやつを選んでいく、一番確率が高いやつ選んでいくので、すごい単純ですけど、こういうトップKeyを取るとかトップ系を取るとかして、最後に一番いいやつを選ぶみたいなことをすると、これも結局計算をたくさんしてることになるわけですね。1個選ぶわけじゃないので、次に評価しなきゃいけないものがどんどん増えていくわけなので、こういった形で増やすっていうのも一つのやり方として存在しています。これ何かDecoding方法の一覧ですけどこれ興味ある人いたら言ってください。"    },
    {
        "content": "どうしようかな。時間も関係あるんでなんか最近の例を一つだけ紹介するとコントラスト美Decodingというのは例えばいまして、Xとリップスティックって書いてあるこの軸にそうとする方法の一つ外部のモデル別のモデルを使う方法なんですけどこの研究によると単純に自分が自分自身のこの出力の確率を使うよりも、なんかしょぼい言語モデルの確率を割ってやったものを使った方が、現地ちょっとログで言うと引いたものですね。方が良い指標になっているということが知られています。これなんか多分僕、なんかよくよく話しすぎちゃうのってよく出てくるものじゃないものについてエキスパートモデルが高く値を出していたLlama5161ってのは普通はあまり出さないわけですけどモデル出さないんだったら、このベースラインとして引いてあるっていうか終わってると良いよっていうようなDecoding方法も出てきています。ここで今つらくSteamDecoding詳細を理解してほしいということよりは、Decodingのときにその他のモデルを使って性能を上げると、要はこのアマチュアモデルの計算がさらに増えるわけですけど、こういった形の概念として新しい手法も出てきていたりしますということで、"    },
    {
        "content": "それからここまでの実行DINGの話は基本的に次のtokenをどう選ぶかという話をしてましたけど、Decoding、次のtokenをどう決めるかだけじゃなくて、全体プロセス全体としてどういうふうな生成を行うかっていうのを一段広く、維持した上から見ましょうというのでMetaGenerationという言い方をするような研究も出てきています。このfromDecodingMetaGenerationというのが、6月かな、に出たサーベイでして、理論人の言語モデルで使うアルゴリズムを広くサーベイしたものになってます。この衣装がMetaGenerationて会員になってますけどちょっとこれがどういうものかについて説明していきたいと思います。そうね三つぐらい種類があるっていうふうに書かれてますられるサービスSEPVersaceリファインメントですけどちょっとこれ何か説明より具体を見た方がわかりやすいと思うんで、それぞれ具体を説明していきたいと思います。最後にこれをずっと振り返ったこういうもんだなと思ってもらえばいいと思います。"    },
    {
        "content": "パレスサーチの方法の一番代表的なのがベストベールと呼ばれるものです。これ要は、これをtokenだと今までの話と同じなんすけど文を何個か生成してみて一番いいやつを選びましょうっていうのが、このパラレルサーチの一番簡単なベストWebっていう方法ですね。一番いいのっていうのがいろいろ考えられて、LLMのスコアを使うっていうのが、これがビール産地とかそういう普通の美味さとかに相当するものですけどベリファイと呼ばれるような学習した評価機を使うような研究だったり、GLUEとか機械翻訳のときにGLUEとか、特定の指標を使うみたいで何かの外部の指標を使って、とりあえずN構成して、一番いいやつを、後で選びましょうとそういうアプローチになってます。ちょっとMBRDecodingの話しようかと思うんすけど時間がだいぶ生BERTかもしれない簡単に言うとこれがMRDecodingというのが最近機械翻訳で注目されてる一つの方法らしいんですけど、この例の中で言うとGLUEとかの指標を使っていいものを選ぶというような研究の例になってます。そうですね伊藤ですけどこの何か期待値の中にいうて書いてあるけど、このUっていうのが、このさっきこのスライドでスコアと、相当してましてGLUEとかMetaとか、いろんなものを使っています。これは実際にこれ人間のサンプルがこういう交換するとこういうコンセプト必要なんですけど、これをこのMBRだとやめていって牝馬の代わりにモデルから生成したものをサンプリングして期待値を取るみたいなことをやるのがMBRDecodingというやれるやり方になってるんすけどちょっと詳細が知りたい人は、だいぶわかりやすい資料が上がってましたのでこのURLから飛んでもらえばと思います。ここではこの、要はスコアを選んでいいものを選ぶというようなやり方でいろいろ発展してきてるんだということを理解していただければいいと思います。"    },
    {
        "content": "それからBestBillonとはちょっと違う方法として、N個を生成した後に、それらを集約するという意味では、Day2Day2Falconシステムをこの枠組みの一つとして入り、あの説明されます。セル仕込ん試験紙は下のようなもんですけど推論のことを応用したものになってて、言語モデルにCoTでのいろんなニーズにパスを出させて、まじないずB'zパスって書いてあるけどそのリズムパス出した後にこの一番よく出てきた答えっていうのを最終的にお答えするというようなやり方になってると思います。これもN個の中から1個選んでるわけじゃないんですけどN個出力して、それを集約する形でこのNCoTはそれぞれ独立に動くのでパラレルに動くので、さらに動かして最後集約するという意味でこのペアパラレルサーチの枠組みの一つの代表的な手法になってます。このアグリゲーションどういうふうにこの結果を集約するか、すいません。だったり、どういうふうにスコアをつけるかっていうので、いろんな手法が知られてましてこれもさっきのサーベイ論文に入っているので興味を引いていけばてもらえればと思います。"    },
    {
        "content": "今のパラレルサーチの中でマジョリティーコーティングとベストWebっていうのが出てきましたけど、これあるタスクこれ確か米数学のタスクだと思うんすけどタスクが載ってないんで興味ある人はこの論文見てもらえばと思うんすけど、例えば比較すると、普通にマジョリティーコーティングするのがこの黒線で、あの青線がこのベストオブLMで終わるって書いてあるけど、これアウトカムSupervisedリワードモデルってもので要は全体に対して、これが正しいか正しくないかを推定するのリワードモデル結果を水の推計するヤードモデルってのを用意してあって、Value&モデルにとって一番良かったやつは最後選ぶっていうそういうやり方になってます。マジョリティーボディーはN個出して最大のものを選ぶっていうやり方です。これやると比べると、リワードモデル使った方がいいよということが反応だと言われています。一応補足なんすよGPTが別にそのリワードモデルとか作る必要ないので簡単な方法であるっていう利点はあって、ベスト弁の方のRMっていうのは、リワードモデル別で学習しなきゃいけないってのが欠点としてあることは一応補足しておきます。"    },
    {
        "content": "それからこのオレンジをスルーして話したんですけど実際この論文はこのオレンジのやり方を提案してる論文になってます。それがこのPRMっていうものでして、PRLってのはプロセスSupervisedリワードモデルっていうふうに呼ばれています。あるいは何かあの論文にあったプロセスリワードモデルって普通に読んでるものもあります。これは全体に対して合ってる間違ってるっていうのを予測するんじゃなくて、このプロセスごとに合ってる間違ってるみたいのを予測するモデルを作る。というものですこれあの数学の問題みたいなプロセスがわかりやすいですけど、最初にレッツコールTheair夢DXみたいな感じでそれぞれのバスが正しいか正しくないかみたいなこれはユーザーが提出してる例ですけど、ユーザーの提出した例を使ってプロセスの正しさを予測してるということをしています。これをやるとさらに性能が上がるっていうことが左の図からわかりまして、これちゃんと説明しなかったですけど横軸がN個なんで生成するものですね数になってますけど、これ増やしていくとさらに性能が上がるということがわかります。このPRみたいなプロセスに対して評価を付けるっていうな説明をしましたけど、こういったやり方をするのがステップレベルサーチっていうふうにさっきのサーベイLaMDAとまとめられています。ごめんなさい。この一つ前の研究ではこのプロセスLayerとモデルのキーワードを使って、あのテスト勉強を選ぶってやり方をしましたけど、ベスト以外にも、このプロセスごとに塊を作って生成していってビームサーチみたいなことをすることもできます。要は最初この2個が良さそうだからこのニコイチに怒らせて、この、こいつからまた発生させて2個作ってまた次のステップ3の選んでみたいな。これを繰り返すっていう方法です。これ丸が一つのプロセスtokenじゃなくてプロセスに対応してましてというのが普通のビルサーチの違いですけどそういった研究もあります。ちなみにそういった研究もありますって言いましたけどこの月Web外っていうのをDay2で多分やったと思うんすけどこれがほぼ似たようなことをしていますみたいなことっていうのはこのリワードモデルっていうの使う代わりに言語モデルで、この状態が戻るいいものかこっからこれゲームを24-0ですけど、達成24になる数字作り方をしてくださいっていうときに、あのもう絶対に達成できない場合は、あの言語モデルがフィールドするというようなことをしているものですけど、そういった形でやってるっていうのも、言語モデルをあのリワードの評価として使っているあの例だとステップレベルサーチのあの例だと、いただければと思います。これもさっきと同様ですけど探索方法とか検証するステップの違いですね。どこですっていう検証するかとかによっていろんな方法が知られています。FSでは先読みしてやるみたいなことです。これもちょっと時間の関係で全体を割愛しますけど、これもさBloomにあるのでやってみようかなと"    },
    {
        "content": "それから全然別のやり方でリファインメントと呼ばれるようなやり方もあります。これ概念図だけピックアップしましたけど、右側の黒いのが、なんかあの、対象の値ってこれを言語モデルが生成したものだとしたときに、これ自身を入れてやって、もっかい生成させるというようなことだったり、あるいはこの生成したものに対してフィードバックを、右側の左側の広い広いボックスに相当しますけど与えてやって再度生成するみたいな、そういったやり方をする研究もありますこれがリファインメントというようなやり方になってます。このリファイベントの代表的な研究がセルフリファインと言われるような研究がありまして左が概念図でさっきとほぼ同じですけど、なんか最初タスクを言語モデルに与えましたとこいつが出力した結果をあのモデル自分自身のこのセリフは研究だと自分自身で評価して、あとフィードバックを返すとそのフィードバックを行った結果を使ってもっかいリファインすると、このリファもこのセリフに入ると自分自身でやるんすけど、そういった枠組みの研究があります。右側例えば政令で上側のABCが一つの系列になってますけど、ダイアログが与えられたと会話が与えられたときに、このフィードバックって書いてあるのが、この左側の①のプロセスに相当するこれも言語モデルが出してるものでこのフィードバックを踏まえてリファインしたのがこの右側のものになります。ちょっと中身は呼ばないですけどこういった形で最初に生成させそれ何を変えるべきかっていうのも言語モデル生成して、最後にその何を変えるべきかというのを踏まえても改正するとこれを何回も何回も繰り返すっていうのはやり方になってます。こんなんで性能上がるのかって思うと思うんですけどこれが何か上がるらしくてですね、最大50パーぐらい上がるような例もあるっていうふうに言われています。結果は結果なので、一旦これぐらいしか"    },
    {
        "content": "これでほぼちょうどですけど、最後に少しあの、前半では全体の訓練時のスケーリングをする話を基本的にしましたけど、最近ではこの推論値の計算量っていうのも注目するような研究が増えてきています。代表的なGPT法案とかですごく注目されてるかなと思いますし、今までやった方法までを学んだ方法も結構出てきたと思いますけど、Promptingを工夫するとか、Decodingを工夫するとかいうので、それにも発展的な方法がいろいろ出てきていますし、まめちMetaGenerationっていうような枠組みで、Decodingだけじゃなくてそのレコードした結果を最後どう使うかみたいな含めて、GENERATIONSMetaGenerationというふうに呼んでますけど、ただヴェルサーチとかステップでVersaceとかリファインメントと言われるような枠組みの研究も出てきていますというような話をしました。"    },
    {
        "content": "最後に二つ補足しておくのパートだろうと思いますけど同じ計算資源のときにパラメータ増やすのよりの推論資源を増やすのが有効なのかっていうのが問いとしてあると思いますけどオープンあの場合だと、訓練時のスケールは同じままっていうのちょっとスケールを増やしたら、より賢くなりましたって話でしたけど、どっちにするのがいいのかっていう意味で言うと、GoogleDeepMindか。1月に論文としてまして、スケーリングLLMthisTimeコンピュート口真理ちゃん日は増えてるっていうことで、良いらしいというふうに言われてます。厳密に言うとこれなんかタスクによって違うということなので、良いとまで言っていいのかちょっと若干誇大広告な気が個人的にはしてますけどそういったことを検証するような研究も出てきていますので興味ある人は見てもらえばと思います。"    }
]